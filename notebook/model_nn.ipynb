{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T11:31:13.726372Z",
     "start_time": "2020-09-09T11:31:11.209097Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T11:33:48.219122Z",
     "start_time": "2020-09-09T11:33:47.871110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\81908\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "ORIG = r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\\Probspace_geme_compe\\data\\orig\"\n",
    "train = pd.read_csv(f\"{ORIG}/train_data.csv\")\n",
    "test = pd.read_csv(f\"{ORIG}/test_data.csv\")\n",
    "df_all = train_df.append(test_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T11:32:58.422481Z",
     "start_time": "2020-09-09T11:32:55.098949Z"
    },
    "code_folding": [
     31,
     42,
     168
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import (\n",
    "    ReLU,\n",
    "    PReLU,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "class ModelNN:\n",
    "    def __init__(self, run_fold_name=\"\", params={}) -> None:\n",
    "        \"\"\"コンストラクタ\n",
    "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前\n",
    "        :param params: ハイパーパラメータ\n",
    "        \"\"\"\n",
    "        self.run_fold_name = run_fold_name\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"モデル構築\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.params[\"units\"][0], input_shape=input_shape))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.params[\"dropout\"][0]))\n",
    "\n",
    "        for l_i in range(1, self.params[\"layers\"] - 1):\n",
    "            model.add(Dense(self.params[\"units\"][l_i]))\n",
    "            model.add(PReLU())\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.params[\"dropout\"][l_i]))\n",
    "\n",
    "        model.add(Dense(self.params[\"nb_classes\"]))\n",
    "        model.add(Activation(self.params[\"pred_activation\"]))\n",
    "        if self.params[\"optimizer\"] == \"adam\":\n",
    "            opt = Adam(learning_rate=self.params[\"learning_rate\"])\n",
    "        else:\n",
    "            opt = SGD(\n",
    "                learning_rate=self.params[\"learning_rate\"], momentum=0.9, nesterov=True\n",
    "            )\n",
    "\n",
    "        model.compile(\n",
    "            loss=self.params[\"loss\"], metrics=self.params[\"metrics\"], optimizer=opt,\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, tr_x, tr_y, va_x=None, va_y=None):\n",
    "        # 乱数固定\n",
    "        ModelNN().set_tf_random_seed()\n",
    "\n",
    "        # 出力ディレクトリ作成\n",
    "        os.makedirs(self.params[\"out_dir\"], exist_ok=True)\n",
    "\n",
    "        # データのセット・スケーリング\n",
    "        validation = va_x is not None\n",
    "        scaler = self.params[\"scaler\"]  # StandardScaler()\n",
    "        scaler.fit(tr_x)\n",
    "        tr_x = scaler.transform(tr_x)\n",
    "        # ラベルone-hot化\n",
    "        tr_y = to_categorical(tr_y, num_classes=self.params[\"nb_classes\"])\n",
    "\n",
    "        # モデル構築\n",
    "        self.build_model((tr_x.shape[1],))\n",
    "        \n",
    "        hist = None\n",
    "        if validation:\n",
    "            va_x = scaler.transform(va_x)\n",
    "            va_y = to_categorical(va_y, num_classes=self.params[\"nb_classes\"])\n",
    "\n",
    "            cb = []\n",
    "            cb.append(\n",
    "                ModelCheckpoint(\n",
    "                    filepath=os.path.join(\n",
    "                        self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"\n",
    "                    ),\n",
    "                    monitor=\"val_loss\",\n",
    "                    save_best_only=True,\n",
    "                    #verbose=1,\n",
    "                    verbose=0,\n",
    "                )\n",
    "            )\n",
    "            # cb.append(ModelCheckpoint(filepath=os.path.join(self.params[\"out_dir\"], f\"best_val_acc_{self.run_fold_name}.h5\"),\n",
    "            #        monitor=\"val_acc\",\n",
    "            #        save_best_only=True,\n",
    "            #        verbose=1,\n",
    "            #        mode=\"max\",\n",
    "            #    )\n",
    "            # )\n",
    "            cb.append(\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=self.params[\"patience\"], verbose=1\n",
    "                )\n",
    "            )\n",
    "            hist = self.model.fit(\n",
    "                tr_x,\n",
    "                tr_y,\n",
    "                epochs=self.params[\"nb_epoch\"],\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                #verbose=2,\n",
    "                verbose=0,\n",
    "                validation_data=(va_x, va_y),\n",
    "                callbacks=cb,\n",
    "            )\n",
    "        else:\n",
    "            hist = self.model.fit(\n",
    "                tr_x,\n",
    "                tr_y,\n",
    "                epochs=self.params[\"nb_epoch\"],\n",
    "                batch_size=self.params[\"batch_size\"],\n",
    "                #verbose=2,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "        # スケーラー保存\n",
    "        self.scaler = scaler\n",
    "        joblib.dump(\n",
    "            self.scaler,\n",
    "            os.path.join(self.params[\"out_dir\"], f\"{self.run_fold_name}-scaler.pkl\"),\n",
    "        )\n",
    "        \n",
    "        # history plot\n",
    "        self.plot_hist_acc_loss(hist)\n",
    "        \n",
    "        return hist\n",
    "\n",
    "    def predict_binary(self, te_x):\n",
    "        \"\"\"2値分類の1クラスのみ取得\"\"\"\n",
    "        self.load_model()\n",
    "        te_x = self.scaler.transform(te_x)\n",
    "        pred = self.model.predict(te_x)[:, 1]\n",
    "        return pred\n",
    "\n",
    "    def load_model(self):\n",
    "        model_path = os.path.join(\n",
    "            self.params[\"out_dir\"], f\"best_val_loss_{self.run_fold_name}.h5\"\n",
    "        )\n",
    "        # model_path = os.path.join(self.params['out_dir'], f'best_val_acc_{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(\n",
    "            self.params[\"out_dir\"], f\"{self.run_fold_name}-scaler.pkl\"\n",
    "        )\n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        print(f\"INFO: \\nload model:{model_path} \\nload scaler: {scaler_path}\")\n",
    "\n",
    "    def plot_hist_acc_loss(self, history):\n",
    "        \"\"\"学習historyをplot\"\"\"\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(len(acc))\n",
    "\n",
    "        # 1) Accracy Plt\n",
    "        plt.plot(epochs, acc, 'bo' ,label = 'training acc')\n",
    "        plt.plot(epochs, val_acc, 'b' , label= 'validation acc')\n",
    "        plt.title('Training and Validation acc')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{self.params['out_dir']}/{self.run_fold_name}-acc.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        # 2) Loss Plt\n",
    "        plt.plot(epochs, loss, 'bo' ,label = 'training loss')\n",
    "        plt.plot(epochs, val_loss, 'b' , label= 'validation loss')\n",
    "        plt.title('Training and Validation loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{self.params['out_dir']}/{self.run_fold_name}-loss.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def set_tf_random_seed(seed=0):\n",
    "        \"\"\"\n",
    "        tensorflow v2.0の乱数固定\n",
    "        https://qiita.com/Rin-P/items/acacbb6bd93d88d1ca1b\n",
    "        ※tensorflow-determinism が無いとgpuについては固定できないみたい\n",
    "         tensorflow-determinism はpipでしか取れない($ pip install tensorflow-determinism)ので未確認\n",
    "        \"\"\"\n",
    "        ## ソースコード上でGPUの計算順序の固定を記述\n",
    "        # from tfdeterminism import patch\n",
    "        # patch()\n",
    "        # 乱数のseed値の固定\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)  # v1.0系だとtf.set_random_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T11:33:11.805852Z",
     "start_time": "2020-09-09T11:33:11.776924Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "class Encoder():\n",
    "    def __init__(self, \n",
    "                 encoder_flags={\"count\": True, \"target\": True, \"catboost\": True, \"label\": True, \"impute_null\": True}) -> None:\n",
    "        self.encoder_flags = encoder_flags\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_encoder(train_df, valid_df, cat_features=None):\n",
    "        \"\"\"\n",
    "        Count_Encoding: カテゴリ列をカウント値に変換する特徴量エンジニアリング（要はgroupby().size()の集計列追加のこと）\n",
    "        ※カウント数が同じカテゴリは同じようなデータ傾向になる可能性がある\n",
    "        https://www.kaggle.com/matleonard/categorical-encodings\n",
    "        \"\"\"\n",
    "        # conda install -c conda-forge category_encoders\n",
    "        import category_encoders as ce\n",
    "\n",
    "        if cat_features is None:\n",
    "            cat_features = train_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "\n",
    "        count_enc = ce.CountEncoder(cols=cat_features)\n",
    "\n",
    "        # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "        count_enc.fit(train_df[cat_features])\n",
    "        train_encoded = train_df.join(\n",
    "            count_enc.transform(train_df[cat_features]).add_suffix(\"_count\")\n",
    "        )\n",
    "        valid_encoded = valid_df.join(\n",
    "            count_enc.transform(valid_df[cat_features]).add_suffix(\"_count\")\n",
    "        )\n",
    "\n",
    "        return train_encoded, valid_encoded\n",
    "\n",
    "    @staticmethod\n",
    "    def target_encoder(train_df, valid_df, target_col: str, cat_features=None):\n",
    "        \"\"\"\n",
    "        Target_Encoding: カテゴリ列を目的変数の平均値に変換する特徴量エンジニアリング\n",
    "        https://www.kaggle.com/matleonard/categorical-encodings\n",
    "        \"\"\"\n",
    "        # conda install -c conda-forge category_encoders\n",
    "        import category_encoders as ce\n",
    "\n",
    "        if cat_features is None:\n",
    "            cat_features = train_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "\n",
    "        target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "        # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "        target_enc.fit(train_df[cat_features], train_df[target_col])\n",
    "\n",
    "        train_encoded = train_df.join(\n",
    "            target_enc.transform(train_df[cat_features]).add_suffix(\"_target\")\n",
    "        )\n",
    "        valid_encoded = valid_df.join(\n",
    "            target_enc.transform(valid_df[cat_features]).add_suffix(\"_target\")\n",
    "        )\n",
    "        return train_encoded, valid_encoded\n",
    "\n",
    "    @staticmethod\n",
    "    def catboost_encoder(train_df, valid_df, target_col: str, cat_features=None):\n",
    "        \"\"\"\n",
    "        CatBoost_Encoding: カテゴリ列を目的変数の1行前の行からのみに変換する特徴量エンジニアリング\n",
    "        CatBoost使ったターゲットエンコーディング\n",
    "        https://www.kaggle.com/matleonard/categorical-encodings\n",
    "        \"\"\"\n",
    "        # conda install -c conda-forge category_encoders\n",
    "        import category_encoders as ce\n",
    "\n",
    "        if cat_features is None:\n",
    "            cat_features = train_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "\n",
    "        cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n",
    "\n",
    "        # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "        cb_enc.fit(train_df[cat_features], train_df[target_col])\n",
    "\n",
    "        train_encoded = train_df.join(\n",
    "            cb_enc.transform(train_df[cat_features]).add_suffix(\"_cb\")\n",
    "        )\n",
    "        valid_encoded = valid_df.join(\n",
    "            cb_enc.transform(valid_df[cat_features]).add_suffix(\"_cb\")\n",
    "        )\n",
    "        return train_encoded, valid_encoded\n",
    "\n",
    "    @staticmethod\n",
    "    def impute_null_add_flag_col(df, strategy=\"median\", cols_with_missing=None, fill_value=None):\n",
    "        \"\"\"欠損値を補間して欠損フラグ列を追加する\n",
    "        fill_value はstrategy=\"constant\"の時のみ有効になる補間する定数\n",
    "        \"\"\"\n",
    "        from sklearn.impute import SimpleImputer\n",
    "\n",
    "        df_plus = df.copy()\n",
    "\n",
    "        if cols_with_missing is None:\n",
    "            if strategy in [\"median\", \"median\"]:\n",
    "                # 数値列で欠損ある列探す\n",
    "                cols_with_missing = [col for col in df.columns if (df[col].isnull().any()) and (df[col].dtype.name not in [\"object\", \"category\", \"bool\"])]\n",
    "            else:\n",
    "                # 欠損ある列探す\n",
    "                cols_with_missing = [col for col in df.columns if (df[col].isnull().any())]\n",
    "\n",
    "        for col in cols_with_missing:\n",
    "            # 欠損フラグ列を追加\n",
    "            #df_plus[col + \"_was_missing\"] = df[col].isnull()\n",
    "            #df_plus[col + \"_was_missing\"] = df_plus[col + \"_was_missing\"].astype(int)\n",
    "            # 欠損値を平均値で補間\n",
    "            my_imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "            df_plus[col] = my_imputer.fit_transform(df[cols_with_missing])\n",
    "\n",
    "        return df_plus\n",
    "    \n",
    "    def run_encoders(self, X, y, train_index, val_index, test_df=None):\n",
    "        \"\"\"カウント,ターゲット,CatBoost,ラベルエンコディング一気にやる。cvの処理のforの中に書きやすいように\"\"\"\n",
    "        train_df = pd.concat([X, y], axis=1)\n",
    "        t_fold_df, v_fold_df = train_df.iloc[train_index], train_df.iloc[val_index]\n",
    "\n",
    "        if self.encoder_flags[\"count\"]:\n",
    "            # カウントエンコディング\n",
    "            t_fold_df, v_fold_df = Encoder().count_encoder(t_fold_df, v_fold_df, cat_features=None)\n",
    "            if df_test is None:\n",
    "                _, test_df = Encoder().count_encoder(t_fold_df, test_df, cat_features=None)\n",
    "        if self.encoder_flags[\"target\"]:\n",
    "            # ターゲットエンコディング\n",
    "            t_fold_df, v_fold_df = Encoder().target_encoder(t_fold_df, v_fold_df, target_col=y.name, cat_features=None)\n",
    "        if self.encoder_flags[\"catboost\"]:\n",
    "            # CatBoostエンコディング\n",
    "            t_fold_df, v_fold_df = Encoder().catboost_encoder(t_fold_df, v_fold_df, target_col=y.name, cat_features=None)\n",
    "        \n",
    "        if self.encoder_flags[\"label\"]:\n",
    "            # ラベルエンコディング\n",
    "            train_df = t_fold_df.append(v_fold_df)  # trainとval再連結\n",
    "            cate_cols = t_fold_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                train_df[col], uni = pd.factorize(train_df[col])\n",
    "                \n",
    "        if self.encoder_flags[\"impute_null\"]:\n",
    "            # 欠損置換（ラベルエンコディングの後じゃないと処理遅くなる）\n",
    "            nulls = train_df.drop(y.name, axis=1).isnull().sum().to_frame()\n",
    "            null_indexs = [index for index, row in nulls.iterrows() if row[0] > 0]\n",
    "            train_df = Encoder().impute_null_add_flag_col(train_df, cols_with_missing=null_indexs, strategy=\"most_frequent\")  # 最頻値で補間\n",
    "        \n",
    "        t_fold_df, v_fold_df = train_df.iloc[train_index], train_df.iloc[val_index]\n",
    "        print(\n",
    "            \"run encoding Train shape: {}, valid shape: {}\".format(\n",
    "                t_fold_df.shape, v_fold_df.shape\n",
    "            )\n",
    "        )\n",
    "        feats = t_fold_df.columns.to_list()\n",
    "        feats.remove(y.name)\n",
    "        X_train, y_train = (t_fold_df[feats], t_fold_df[y.name])\n",
    "        X_val, y_val = (v_fold_df[feats], v_fold_df[y.name])\n",
    "        return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T13:15:04.407233Z",
     "start_time": "2020-09-09T13:11:44.265766Z"
    },
    "code_folding": [
     65
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_features ['A1-rank', 'A1-weapon', 'A2-rank', 'A2-weapon', 'A3-rank', 'A3-weapon', 'A4-rank', 'A4-weapon', 'B1-rank', 'B1-weapon', 'B2-rank', 'B2-weapon', 'B3-rank', 'B3-weapon', 'B4-rank', 'B4-weapon', 'lobby', 'lobby-mode', 'mode', 'period', 'stage']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1-level</th>\n",
       "      <th>A1-rank</th>\n",
       "      <th>A1-weapon</th>\n",
       "      <th>A2-level</th>\n",
       "      <th>A2-rank</th>\n",
       "      <th>A2-weapon</th>\n",
       "      <th>A3-level</th>\n",
       "      <th>A3-rank</th>\n",
       "      <th>A3-weapon</th>\n",
       "      <th>A4-level</th>\n",
       "      <th>...</th>\n",
       "      <th>B3-weapon</th>\n",
       "      <th>B4-level</th>\n",
       "      <th>B4-rank</th>\n",
       "      <th>B4-weapon</th>\n",
       "      <th>lobby</th>\n",
       "      <th>lobby-mode</th>\n",
       "      <th>mode</th>\n",
       "      <th>period</th>\n",
       "      <th>stage</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>198.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1-level  A1-rank  A1-weapon  A2-level  A2-rank  A2-weapon  A3-level  \\\n",
       "0       139       -1          0     118.0       -1          0      13.0   \n",
       "1       198       -1          1      77.0       -1          1     198.0   \n",
       "\n",
       "   A3-rank  A3-weapon  A4-level  ...  B3-weapon  B4-level  B4-rank  B4-weapon  \\\n",
       "0       -1          0      10.0  ...          0      10.0       -1          0   \n",
       "1       -1          1     123.0  ...          1     123.0       -1          1   \n",
       "\n",
       "   lobby  lobby-mode  mode  period  stage    y  \n",
       "0      0           0     0       0      0  1.0  \n",
       "1      0           0     0       1      1  0.0  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fold: 0\n",
      "before enc_df: (52900, 30) (13225, 30) (28340, 29)\n",
      "after enc_df: (52900, 72) (13225, 72) (28340, 71)\n",
      "Train on 52900 samples, validate on 13225 samples\n",
      "Epoch 1/100\n",
      "52900/52900 - 2s - loss: 0.8301 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5201\n",
      "Epoch 2/100\n",
      "52900/52900 - 1s - loss: 0.7087 - acc: 0.5133 - val_loss: 0.6912 - val_acc: 0.5226\n",
      "Epoch 3/100\n",
      "52900/52900 - 1s - loss: 0.6920 - acc: 0.5273 - val_loss: 0.6904 - val_acc: 0.5250\n",
      "Epoch 4/100\n",
      "52900/52900 - 1s - loss: 0.6908 - acc: 0.5283 - val_loss: 0.6905 - val_acc: 0.5227\n",
      "Epoch 5/100\n",
      "52900/52900 - 1s - loss: 0.6897 - acc: 0.5314 - val_loss: 0.6894 - val_acc: 0.5326\n",
      "Epoch 6/100\n",
      "52900/52900 - 1s - loss: 0.6896 - acc: 0.5304 - val_loss: 0.6899 - val_acc: 0.5290\n",
      "Epoch 7/100\n",
      "52900/52900 - 1s - loss: 0.6895 - acc: 0.5321 - val_loss: 0.6908 - val_acc: 0.5270\n",
      "Epoch 8/100\n",
      "52900/52900 - 1s - loss: 0.6893 - acc: 0.5325 - val_loss: 0.6893 - val_acc: 0.5313\n",
      "Epoch 9/100\n",
      "52900/52900 - 1s - loss: 0.6890 - acc: 0.5332 - val_loss: 0.6899 - val_acc: 0.5317\n",
      "Epoch 10/100\n",
      "52900/52900 - 1s - loss: 0.6887 - acc: 0.5337 - val_loss: 0.6898 - val_acc: 0.5319\n",
      "Epoch 11/100\n",
      "52900/52900 - 1s - loss: 0.6888 - acc: 0.5328 - val_loss: 0.6901 - val_acc: 0.5305\n",
      "Epoch 12/100\n",
      "52900/52900 - 1s - loss: 0.6887 - acc: 0.5329 - val_loss: 0.6891 - val_acc: 0.5315\n",
      "Epoch 13/100\n",
      "52900/52900 - 1s - loss: 0.6881 - acc: 0.5350 - val_loss: 0.6900 - val_acc: 0.5290\n",
      "Epoch 14/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5383 - val_loss: 0.6897 - val_acc: 0.5328\n",
      "Epoch 15/100\n",
      "52900/52900 - 1s - loss: 0.6879 - acc: 0.5375 - val_loss: 0.6896 - val_acc: 0.5262\n",
      "Epoch 16/100\n",
      "52900/52900 - 1s - loss: 0.6875 - acc: 0.5384 - val_loss: 0.6900 - val_acc: 0.5226\n",
      "Epoch 17/100\n",
      "52900/52900 - 1s - loss: 0.6873 - acc: 0.5410 - val_loss: 0.6906 - val_acc: 0.5259\n",
      "Epoch 18/100\n",
      "52900/52900 - 1s - loss: 0.6866 - acc: 0.5415 - val_loss: 0.6903 - val_acc: 0.5261\n",
      "Epoch 19/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5426 - val_loss: 0.6906 - val_acc: 0.5309\n",
      "Epoch 20/100\n",
      "52900/52900 - 1s - loss: 0.6862 - acc: 0.5465 - val_loss: 0.6907 - val_acc: 0.5279\n",
      "Epoch 21/100\n",
      "52900/52900 - 1s - loss: 0.6862 - acc: 0.5438 - val_loss: 0.6908 - val_acc: 0.5266\n",
      "Epoch 22/100\n",
      "52900/52900 - 1s - loss: 0.6858 - acc: 0.5440 - val_loss: 0.6908 - val_acc: 0.5310\n",
      "Epoch 23/100\n",
      "52900/52900 - 1s - loss: 0.6851 - acc: 0.5478 - val_loss: 0.6906 - val_acc: 0.5257\n",
      "Epoch 24/100\n",
      "52900/52900 - 1s - loss: 0.6847 - acc: 0.5509 - val_loss: 0.6919 - val_acc: 0.5205\n",
      "Epoch 25/100\n",
      "52900/52900 - 1s - loss: 0.6838 - acc: 0.5532 - val_loss: 0.6923 - val_acc: 0.5237\n",
      "Epoch 26/100\n",
      "52900/52900 - 1s - loss: 0.6835 - acc: 0.5520 - val_loss: 0.6926 - val_acc: 0.5216\n",
      "Epoch 27/100\n",
      "52900/52900 - 1s - loss: 0.6833 - acc: 0.5539 - val_loss: 0.6918 - val_acc: 0.5218\n",
      "Epoch 28/100\n",
      "52900/52900 - 1s - loss: 0.6831 - acc: 0.5563 - val_loss: 0.6931 - val_acc: 0.5217\n",
      "Epoch 29/100\n",
      "52900/52900 - 1s - loss: 0.6826 - acc: 0.5554 - val_loss: 0.6935 - val_acc: 0.5164\n",
      "Epoch 30/100\n",
      "52900/52900 - 1s - loss: 0.6819 - acc: 0.5607 - val_loss: 0.6934 - val_acc: 0.5244\n",
      "Epoch 31/100\n",
      "52900/52900 - 1s - loss: 0.6814 - acc: 0.5592 - val_loss: 0.6930 - val_acc: 0.5209\n",
      "Epoch 32/100\n",
      "52900/52900 - 1s - loss: 0.6802 - acc: 0.5635 - val_loss: 0.6947 - val_acc: 0.5197\n",
      "Epoch 33/100\n",
      "52900/52900 - 1s - loss: 0.6809 - acc: 0.5614 - val_loss: 0.6947 - val_acc: 0.5166\n",
      "Epoch 34/100\n",
      "52900/52900 - 1s - loss: 0.6790 - acc: 0.5676 - val_loss: 0.6950 - val_acc: 0.5198\n",
      "Epoch 35/100\n",
      "52900/52900 - 1s - loss: 0.6789 - acc: 0.5667 - val_loss: 0.6954 - val_acc: 0.5202\n",
      "Epoch 36/100\n",
      "52900/52900 - 1s - loss: 0.6780 - acc: 0.5692 - val_loss: 0.6960 - val_acc: 0.5206\n",
      "Epoch 37/100\n",
      "52900/52900 - 1s - loss: 0.6776 - acc: 0.5704 - val_loss: 0.6963 - val_acc: 0.5198\n",
      "Epoch 38/100\n",
      "52900/52900 - 1s - loss: 0.6779 - acc: 0.5712 - val_loss: 0.6954 - val_acc: 0.5165\n",
      "Epoch 39/100\n",
      "52900/52900 - 1s - loss: 0.6764 - acc: 0.5725 - val_loss: 0.6955 - val_acc: 0.5254\n",
      "Epoch 40/100\n",
      "52900/52900 - 1s - loss: 0.6763 - acc: 0.5723 - val_loss: 0.6963 - val_acc: 0.5183\n",
      "Epoch 41/100\n",
      "52900/52900 - 1s - loss: 0.6765 - acc: 0.5716 - val_loss: 0.6960 - val_acc: 0.5180\n",
      "Epoch 42/100\n",
      "52900/52900 - 1s - loss: 0.6767 - acc: 0.5743 - val_loss: 0.6961 - val_acc: 0.5229\n",
      "Epoch 00042: early stopping\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_0.h5 \n",
      "load scaler: tmp\\0-scaler.pkl\n",
      "valid_pred: [0.5871     0.5072114  0.4976955  ... 0.5301033  0.51453143 0.5254699 ]\n",
      "fold=5, acc=0.5314933837429111\n",
      "\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_0.h5 \n",
      "load scaler: tmp\\0-scaler.pkl\n",
      "test_pred: [0.4944153  0.49529764 0.49451798 ... 0.5213459  0.49516356 0.4768307 ] \n",
      "\n",
      "n_fold: 1\n",
      "before enc_df: (52900, 30) (13225, 30) (28340, 29)\n",
      "after enc_df: (52900, 72) (13225, 72) (28340, 71)\n",
      "Train on 52900 samples, validate on 13225 samples\n",
      "Epoch 1/100\n",
      "52900/52900 - 2s - loss: 0.8289 - acc: 0.5051 - val_loss: 0.6910 - val_acc: 0.5305\n",
      "Epoch 2/100\n",
      "52900/52900 - 1s - loss: 0.7085 - acc: 0.5142 - val_loss: 0.6897 - val_acc: 0.5304\n",
      "Epoch 3/100\n",
      "52900/52900 - 1s - loss: 0.6931 - acc: 0.5245 - val_loss: 0.6890 - val_acc: 0.5322\n",
      "Epoch 4/100\n",
      "52900/52900 - 1s - loss: 0.6912 - acc: 0.5221 - val_loss: 0.6895 - val_acc: 0.5304\n",
      "Epoch 5/100\n",
      "52900/52900 - 1s - loss: 0.6906 - acc: 0.5265 - val_loss: 0.6890 - val_acc: 0.5322\n",
      "Epoch 6/100\n",
      "52900/52900 - 1s - loss: 0.6900 - acc: 0.5278 - val_loss: 0.6892 - val_acc: 0.5331\n",
      "Epoch 7/100\n",
      "52900/52900 - 1s - loss: 0.6894 - acc: 0.5293 - val_loss: 0.6893 - val_acc: 0.5293\n",
      "Epoch 8/100\n",
      "52900/52900 - 1s - loss: 0.6894 - acc: 0.5330 - val_loss: 0.6893 - val_acc: 0.5329\n",
      "Epoch 9/100\n",
      "52900/52900 - 1s - loss: 0.6895 - acc: 0.5308 - val_loss: 0.6894 - val_acc: 0.5346\n",
      "Epoch 10/100\n",
      "52900/52900 - 1s - loss: 0.6890 - acc: 0.5314 - val_loss: 0.6911 - val_acc: 0.5309\n",
      "Epoch 11/100\n",
      "52900/52900 - 1s - loss: 0.6890 - acc: 0.5360 - val_loss: 0.6891 - val_acc: 0.5352\n",
      "Epoch 12/100\n",
      "52900/52900 - 1s - loss: 0.6884 - acc: 0.5358 - val_loss: 0.6893 - val_acc: 0.5285\n",
      "Epoch 13/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5364 - val_loss: 0.6900 - val_acc: 0.5299\n",
      "Epoch 14/100\n",
      "52900/52900 - 1s - loss: 0.6887 - acc: 0.5344 - val_loss: 0.6893 - val_acc: 0.5316\n",
      "Epoch 15/100\n",
      "52900/52900 - 1s - loss: 0.6876 - acc: 0.5408 - val_loss: 0.6899 - val_acc: 0.5245\n",
      "Epoch 16/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5352 - val_loss: 0.6891 - val_acc: 0.5282\n",
      "Epoch 17/100\n",
      "52900/52900 - 1s - loss: 0.6871 - acc: 0.5417 - val_loss: 0.6908 - val_acc: 0.5246\n",
      "Epoch 18/100\n",
      "52900/52900 - 1s - loss: 0.6869 - acc: 0.5425 - val_loss: 0.6894 - val_acc: 0.5279\n",
      "Epoch 19/100\n",
      "52900/52900 - 1s - loss: 0.6873 - acc: 0.5405 - val_loss: 0.6895 - val_acc: 0.5276\n",
      "Epoch 20/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5420 - val_loss: 0.6894 - val_acc: 0.5313\n",
      "Epoch 21/100\n",
      "52900/52900 - 1s - loss: 0.6862 - acc: 0.5436 - val_loss: 0.6899 - val_acc: 0.5285\n",
      "Epoch 22/100\n",
      "52900/52900 - 1s - loss: 0.6854 - acc: 0.5487 - val_loss: 0.6901 - val_acc: 0.5271\n",
      "Epoch 23/100\n",
      "52900/52900 - 1s - loss: 0.6859 - acc: 0.5444 - val_loss: 0.6902 - val_acc: 0.5252\n",
      "Epoch 24/100\n",
      "52900/52900 - 1s - loss: 0.6857 - acc: 0.5459 - val_loss: 0.6900 - val_acc: 0.5275\n",
      "Epoch 25/100\n",
      "52900/52900 - 1s - loss: 0.6847 - acc: 0.5459 - val_loss: 0.6906 - val_acc: 0.5265\n",
      "Epoch 26/100\n",
      "52900/52900 - 1s - loss: 0.6850 - acc: 0.5493 - val_loss: 0.6910 - val_acc: 0.5247\n",
      "Epoch 27/100\n",
      "52900/52900 - 1s - loss: 0.6845 - acc: 0.5518 - val_loss: 0.6905 - val_acc: 0.5249\n",
      "Epoch 28/100\n",
      "52900/52900 - 1s - loss: 0.6833 - acc: 0.5532 - val_loss: 0.6902 - val_acc: 0.5264\n",
      "Epoch 29/100\n",
      "52900/52900 - 1s - loss: 0.6827 - acc: 0.5572 - val_loss: 0.6922 - val_acc: 0.5255\n",
      "Epoch 30/100\n",
      "52900/52900 - 1s - loss: 0.6830 - acc: 0.5560 - val_loss: 0.6906 - val_acc: 0.5245\n",
      "Epoch 31/100\n",
      "52900/52900 - 1s - loss: 0.6828 - acc: 0.5560 - val_loss: 0.6916 - val_acc: 0.5282\n",
      "Epoch 32/100\n",
      "52900/52900 - 1s - loss: 0.6816 - acc: 0.5580 - val_loss: 0.6919 - val_acc: 0.5248\n",
      "Epoch 33/100\n",
      "52900/52900 - 1s - loss: 0.6816 - acc: 0.5563 - val_loss: 0.6913 - val_acc: 0.5282\n",
      "Epoch 00033: early stopping\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_1.h5 \n",
      "load scaler: tmp\\1-scaler.pkl\n",
      "valid_pred: [0.56584996 0.54083115 0.5729148  ... 0.52885044 0.5064841  0.53388625]\n",
      "fold=5, acc=0.5321739130434783\n",
      "\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_1.h5 \n",
      "load scaler: tmp\\1-scaler.pkl\n",
      "test_pred: [0.49094975 0.4911501  0.52036124 ... 0.5410652  0.48353535 0.47746512] \n",
      "\n",
      "n_fold: 2\n",
      "before enc_df: (52900, 30) (13225, 30) (28340, 29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after enc_df: (52900, 72) (13225, 72) (28340, 71)\n",
      "Train on 52900 samples, validate on 13225 samples\n",
      "Epoch 1/100\n",
      "52900/52900 - 2s - loss: 0.8270 - acc: 0.5092 - val_loss: 0.6913 - val_acc: 0.5243\n",
      "Epoch 2/100\n",
      "52900/52900 - 1s - loss: 0.7070 - acc: 0.5183 - val_loss: 0.6898 - val_acc: 0.5287\n",
      "Epoch 3/100\n",
      "52900/52900 - 1s - loss: 0.6926 - acc: 0.5257 - val_loss: 0.6904 - val_acc: 0.5240\n",
      "Epoch 4/100\n",
      "52900/52900 - 1s - loss: 0.6910 - acc: 0.5253 - val_loss: 0.6897 - val_acc: 0.5262\n",
      "Epoch 5/100\n",
      "52900/52900 - 1s - loss: 0.6897 - acc: 0.5285 - val_loss: 0.6892 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "52900/52900 - 1s - loss: 0.6900 - acc: 0.5284 - val_loss: 0.6898 - val_acc: 0.5254\n",
      "Epoch 7/100\n",
      "52900/52900 - 1s - loss: 0.6897 - acc: 0.5304 - val_loss: 0.6904 - val_acc: 0.5255\n",
      "Epoch 8/100\n",
      "52900/52900 - 1s - loss: 0.6897 - acc: 0.5321 - val_loss: 0.6886 - val_acc: 0.5293\n",
      "Epoch 9/100\n",
      "52900/52900 - 1s - loss: 0.6893 - acc: 0.5327 - val_loss: 0.6894 - val_acc: 0.5257\n",
      "Epoch 10/100\n",
      "52900/52900 - 1s - loss: 0.6889 - acc: 0.5353 - val_loss: 0.6888 - val_acc: 0.5269\n",
      "Epoch 11/100\n",
      "52900/52900 - 1s - loss: 0.6885 - acc: 0.5354 - val_loss: 0.6894 - val_acc: 0.5248\n",
      "Epoch 12/100\n",
      "52900/52900 - 1s - loss: 0.6885 - acc: 0.5342 - val_loss: 0.6893 - val_acc: 0.5294\n",
      "Epoch 13/100\n",
      "52900/52900 - 1s - loss: 0.6884 - acc: 0.5354 - val_loss: 0.6899 - val_acc: 0.5236\n",
      "Epoch 14/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5375 - val_loss: 0.6887 - val_acc: 0.5275\n",
      "Epoch 15/100\n",
      "52900/52900 - 1s - loss: 0.6881 - acc: 0.5367 - val_loss: 0.6892 - val_acc: 0.5265\n",
      "Epoch 16/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5399 - val_loss: 0.6896 - val_acc: 0.5229\n",
      "Epoch 17/100\n",
      "52900/52900 - 1s - loss: 0.6877 - acc: 0.5402 - val_loss: 0.6893 - val_acc: 0.5245\n",
      "Epoch 18/100\n",
      "52900/52900 - 1s - loss: 0.6870 - acc: 0.5419 - val_loss: 0.6891 - val_acc: 0.5217\n",
      "Epoch 19/100\n",
      "52900/52900 - 1s - loss: 0.6872 - acc: 0.5416 - val_loss: 0.6890 - val_acc: 0.5249\n",
      "Epoch 20/100\n",
      "52900/52900 - 1s - loss: 0.6870 - acc: 0.5439 - val_loss: 0.6908 - val_acc: 0.5189\n",
      "Epoch 21/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5443 - val_loss: 0.6890 - val_acc: 0.5289\n",
      "Epoch 22/100\n",
      "52900/52900 - 1s - loss: 0.6862 - acc: 0.5458 - val_loss: 0.6897 - val_acc: 0.5286\n",
      "Epoch 23/100\n",
      "52900/52900 - 1s - loss: 0.6854 - acc: 0.5453 - val_loss: 0.6903 - val_acc: 0.5284\n",
      "Epoch 24/100\n",
      "52900/52900 - 1s - loss: 0.6853 - acc: 0.5488 - val_loss: 0.6893 - val_acc: 0.5310\n",
      "Epoch 25/100\n",
      "52900/52900 - 1s - loss: 0.6850 - acc: 0.5503 - val_loss: 0.6894 - val_acc: 0.5276\n",
      "Epoch 26/100\n",
      "52900/52900 - 1s - loss: 0.6848 - acc: 0.5518 - val_loss: 0.6896 - val_acc: 0.5276\n",
      "Epoch 27/100\n",
      "52900/52900 - 1s - loss: 0.6844 - acc: 0.5468 - val_loss: 0.6903 - val_acc: 0.5256\n",
      "Epoch 28/100\n",
      "52900/52900 - 1s - loss: 0.6842 - acc: 0.5526 - val_loss: 0.6900 - val_acc: 0.5293\n",
      "Epoch 29/100\n",
      "52900/52900 - 1s - loss: 0.6832 - acc: 0.5534 - val_loss: 0.6905 - val_acc: 0.5211\n",
      "Epoch 30/100\n",
      "52900/52900 - 1s - loss: 0.6834 - acc: 0.5544 - val_loss: 0.6915 - val_acc: 0.5233\n",
      "Epoch 31/100\n",
      "52900/52900 - 1s - loss: 0.6830 - acc: 0.5556 - val_loss: 0.6906 - val_acc: 0.5272\n",
      "Epoch 32/100\n",
      "52900/52900 - 1s - loss: 0.6817 - acc: 0.5584 - val_loss: 0.6900 - val_acc: 0.5286\n",
      "Epoch 33/100\n",
      "52900/52900 - 1s - loss: 0.6812 - acc: 0.5588 - val_loss: 0.6915 - val_acc: 0.5247\n",
      "Epoch 34/100\n",
      "52900/52900 - 1s - loss: 0.6813 - acc: 0.5597 - val_loss: 0.6920 - val_acc: 0.5241\n",
      "Epoch 35/100\n",
      "52900/52900 - 1s - loss: 0.6812 - acc: 0.5604 - val_loss: 0.6920 - val_acc: 0.5264\n",
      "Epoch 36/100\n",
      "52900/52900 - 1s - loss: 0.6808 - acc: 0.5585 - val_loss: 0.6917 - val_acc: 0.5252\n",
      "Epoch 37/100\n",
      "52900/52900 - 1s - loss: 0.6811 - acc: 0.5613 - val_loss: 0.6920 - val_acc: 0.5274\n",
      "Epoch 38/100\n",
      "52900/52900 - 1s - loss: 0.6796 - acc: 0.5644 - val_loss: 0.6927 - val_acc: 0.5217\n",
      "Epoch 00038: early stopping\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_2.h5 \n",
      "load scaler: tmp\\2-scaler.pkl\n",
      "valid_pred: [0.55519706 0.53034025 0.50448775 ... 0.50614107 0.47201145 0.5364305 ]\n",
      "fold=5, acc=0.5293005671077504\n",
      "\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_2.h5 \n",
      "load scaler: tmp\\2-scaler.pkl\n",
      "test_pred: [0.47864744 0.49069798 0.49926284 ... 0.55477    0.4595545  0.47510242] \n",
      "\n",
      "n_fold: 3\n",
      "before enc_df: (52900, 30) (13225, 30) (28340, 29)\n",
      "after enc_df: (52900, 72) (13225, 72) (28340, 71)\n",
      "Train on 52900 samples, validate on 13225 samples\n",
      "Epoch 1/100\n",
      "52900/52900 - 2s - loss: 0.8242 - acc: 0.5100 - val_loss: 0.6921 - val_acc: 0.5232\n",
      "Epoch 2/100\n",
      "52900/52900 - 1s - loss: 0.7096 - acc: 0.5115 - val_loss: 0.6908 - val_acc: 0.5195\n",
      "Epoch 3/100\n",
      "52900/52900 - 1s - loss: 0.6931 - acc: 0.5229 - val_loss: 0.6896 - val_acc: 0.5267\n",
      "Epoch 4/100\n",
      "52900/52900 - 1s - loss: 0.6908 - acc: 0.5261 - val_loss: 0.6901 - val_acc: 0.5254\n",
      "Epoch 5/100\n",
      "52900/52900 - 1s - loss: 0.6901 - acc: 0.5284 - val_loss: 0.6896 - val_acc: 0.5236\n",
      "Epoch 6/100\n",
      "52900/52900 - 1s - loss: 0.6902 - acc: 0.5278 - val_loss: 0.6893 - val_acc: 0.5286\n",
      "Epoch 7/100\n",
      "52900/52900 - 1s - loss: 0.6899 - acc: 0.5299 - val_loss: 0.6898 - val_acc: 0.5280\n",
      "Epoch 8/100\n",
      "52900/52900 - 1s - loss: 0.6893 - acc: 0.5323 - val_loss: 0.6888 - val_acc: 0.5295\n",
      "Epoch 9/100\n",
      "52900/52900 - 1s - loss: 0.6895 - acc: 0.5350 - val_loss: 0.6888 - val_acc: 0.5272\n",
      "Epoch 10/100\n",
      "52900/52900 - 1s - loss: 0.6889 - acc: 0.5358 - val_loss: 0.6888 - val_acc: 0.5253\n",
      "Epoch 11/100\n",
      "52900/52900 - 1s - loss: 0.6885 - acc: 0.5348 - val_loss: 0.6889 - val_acc: 0.5249\n",
      "Epoch 12/100\n",
      "52900/52900 - 1s - loss: 0.6889 - acc: 0.5329 - val_loss: 0.6895 - val_acc: 0.5324\n",
      "Epoch 13/100\n",
      "52900/52900 - 1s - loss: 0.6886 - acc: 0.5357 - val_loss: 0.6909 - val_acc: 0.5199\n",
      "Epoch 14/100\n",
      "52900/52900 - 1s - loss: 0.6883 - acc: 0.5365 - val_loss: 0.6895 - val_acc: 0.5265\n",
      "Epoch 15/100\n",
      "52900/52900 - 1s - loss: 0.6880 - acc: 0.5360 - val_loss: 0.6895 - val_acc: 0.5253\n",
      "Epoch 16/100\n",
      "52900/52900 - 1s - loss: 0.6876 - acc: 0.5389 - val_loss: 0.6889 - val_acc: 0.5319\n",
      "Epoch 17/100\n",
      "52900/52900 - 1s - loss: 0.6876 - acc: 0.5409 - val_loss: 0.6902 - val_acc: 0.5218\n",
      "Epoch 18/100\n",
      "52900/52900 - 1s - loss: 0.6870 - acc: 0.5410 - val_loss: 0.6896 - val_acc: 0.5321\n",
      "Epoch 19/100\n",
      "52900/52900 - 1s - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6894 - val_acc: 0.5282\n",
      "Epoch 20/100\n",
      "52900/52900 - 1s - loss: 0.6866 - acc: 0.5420 - val_loss: 0.6900 - val_acc: 0.5251\n",
      "Epoch 21/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5439 - val_loss: 0.6900 - val_acc: 0.5257\n",
      "Epoch 22/100\n",
      "52900/52900 - 1s - loss: 0.6864 - acc: 0.5426 - val_loss: 0.6890 - val_acc: 0.5312\n",
      "Epoch 23/100\n",
      "52900/52900 - 1s - loss: 0.6857 - acc: 0.5453 - val_loss: 0.6908 - val_acc: 0.5249\n",
      "Epoch 24/100\n",
      "52900/52900 - 1s - loss: 0.6854 - acc: 0.5467 - val_loss: 0.6900 - val_acc: 0.5314\n",
      "Epoch 25/100\n",
      "52900/52900 - 1s - loss: 0.6847 - acc: 0.5503 - val_loss: 0.6910 - val_acc: 0.5260\n",
      "Epoch 26/100\n",
      "52900/52900 - 1s - loss: 0.6849 - acc: 0.5483 - val_loss: 0.6919 - val_acc: 0.5239\n",
      "Epoch 27/100\n",
      "52900/52900 - 1s - loss: 0.6839 - acc: 0.5502 - val_loss: 0.6912 - val_acc: 0.5266\n",
      "Epoch 28/100\n",
      "52900/52900 - 1s - loss: 0.6842 - acc: 0.5498 - val_loss: 0.6906 - val_acc: 0.5251\n",
      "Epoch 29/100\n",
      "52900/52900 - 1s - loss: 0.6830 - acc: 0.5561 - val_loss: 0.6906 - val_acc: 0.5283\n",
      "Epoch 30/100\n",
      "52900/52900 - 1s - loss: 0.6833 - acc: 0.5523 - val_loss: 0.6915 - val_acc: 0.5267\n",
      "Epoch 31/100\n",
      "52900/52900 - 1s - loss: 0.6825 - acc: 0.5573 - val_loss: 0.6920 - val_acc: 0.5276\n",
      "Epoch 32/100\n",
      "52900/52900 - 1s - loss: 0.6818 - acc: 0.5590 - val_loss: 0.6919 - val_acc: 0.5261\n",
      "Epoch 33/100\n",
      "52900/52900 - 1s - loss: 0.6817 - acc: 0.5611 - val_loss: 0.6924 - val_acc: 0.5242\n",
      "Epoch 34/100\n",
      "52900/52900 - 1s - loss: 0.6809 - acc: 0.5595 - val_loss: 0.6933 - val_acc: 0.5254\n",
      "Epoch 35/100\n",
      "52900/52900 - 1s - loss: 0.6803 - acc: 0.5617 - val_loss: 0.6930 - val_acc: 0.5222\n",
      "Epoch 36/100\n",
      "52900/52900 - 1s - loss: 0.6799 - acc: 0.5620 - val_loss: 0.6937 - val_acc: 0.5259\n",
      "Epoch 37/100\n",
      "52900/52900 - 1s - loss: 0.6794 - acc: 0.5654 - val_loss: 0.6933 - val_acc: 0.5229\n",
      "Epoch 38/100\n",
      "52900/52900 - 1s - loss: 0.6782 - acc: 0.5651 - val_loss: 0.6946 - val_acc: 0.5211\n",
      "Epoch 39/100\n",
      "52900/52900 - 1s - loss: 0.6798 - acc: 0.5647 - val_loss: 0.6930 - val_acc: 0.5214\n",
      "Epoch 00039: early stopping\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_3.h5 \n",
      "load scaler: tmp\\3-scaler.pkl\n",
      "valid_pred: [0.50095624 0.5051458  0.54004836 ... 0.522699   0.6194509  0.6164869 ]\n",
      "fold=5, acc=0.5271833648393195\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "load model:tmp\\best_val_loss_3.h5 \n",
      "load scaler: tmp\\3-scaler.pkl\n",
      "test_pred: [0.50933087 0.50084597 0.51514995 ... 0.5353369  0.48810425 0.50204545] \n",
      "\n",
      "n_fold: 4\n",
      "before enc_df: (52900, 30) (13225, 30) (28340, 29)\n",
      "after enc_df: (52900, 72) (13225, 72) (28340, 71)\n",
      "Train on 52900 samples, validate on 13225 samples\n",
      "Epoch 1/100\n",
      "52900/52900 - 2s - loss: 0.8282 - acc: 0.5072 - val_loss: 0.6924 - val_acc: 0.5240\n",
      "Epoch 2/100\n",
      "52900/52900 - 1s - loss: 0.7065 - acc: 0.5175 - val_loss: 0.6903 - val_acc: 0.5261\n",
      "Epoch 3/100\n",
      "52900/52900 - 1s - loss: 0.6924 - acc: 0.5237 - val_loss: 0.6899 - val_acc: 0.5284\n",
      "Epoch 4/100\n",
      "52900/52900 - 1s - loss: 0.6904 - acc: 0.5279 - val_loss: 0.6900 - val_acc: 0.5288\n",
      "Epoch 5/100\n",
      "52900/52900 - 1s - loss: 0.6900 - acc: 0.5282 - val_loss: 0.6902 - val_acc: 0.5265\n",
      "Epoch 6/100\n",
      "52900/52900 - 1s - loss: 0.6896 - acc: 0.5283 - val_loss: 0.6903 - val_acc: 0.5291\n",
      "Epoch 7/100\n",
      "52900/52900 - 1s - loss: 0.6895 - acc: 0.5305 - val_loss: 0.6896 - val_acc: 0.5284\n",
      "Epoch 8/100\n",
      "52900/52900 - 1s - loss: 0.6890 - acc: 0.5327 - val_loss: 0.6898 - val_acc: 0.5260\n",
      "Epoch 9/100\n",
      "52900/52900 - 1s - loss: 0.6888 - acc: 0.5338 - val_loss: 0.6899 - val_acc: 0.5272\n",
      "Epoch 10/100\n",
      "52900/52900 - 1s - loss: 0.6886 - acc: 0.5347 - val_loss: 0.6907 - val_acc: 0.5271\n",
      "Epoch 11/100\n",
      "52900/52900 - 1s - loss: 0.6885 - acc: 0.5344 - val_loss: 0.6908 - val_acc: 0.5263\n",
      "Epoch 12/100\n",
      "52900/52900 - 1s - loss: 0.6878 - acc: 0.5379 - val_loss: 0.6904 - val_acc: 0.5245\n",
      "Epoch 13/100\n",
      "52900/52900 - 1s - loss: 0.6884 - acc: 0.5370 - val_loss: 0.6901 - val_acc: 0.5301\n",
      "Epoch 14/100\n",
      "52900/52900 - 1s - loss: 0.6876 - acc: 0.5378 - val_loss: 0.6915 - val_acc: 0.5245\n",
      "Epoch 15/100\n",
      "52900/52900 - 1s - loss: 0.6872 - acc: 0.5381 - val_loss: 0.6904 - val_acc: 0.5267\n",
      "Epoch 16/100\n",
      "52900/52900 - 1s - loss: 0.6871 - acc: 0.5404 - val_loss: 0.6904 - val_acc: 0.5260\n",
      "Epoch 17/100\n",
      "52900/52900 - 1s - loss: 0.6875 - acc: 0.5410 - val_loss: 0.6905 - val_acc: 0.5298\n",
      "Epoch 18/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5430 - val_loss: 0.6902 - val_acc: 0.5273\n",
      "Epoch 19/100\n",
      "52900/52900 - 1s - loss: 0.6865 - acc: 0.5415 - val_loss: 0.6911 - val_acc: 0.5273\n",
      "Epoch 20/100\n",
      "52900/52900 - 1s - loss: 0.6856 - acc: 0.5470 - val_loss: 0.6911 - val_acc: 0.5295\n",
      "Epoch 21/100\n",
      "52900/52900 - 1s - loss: 0.6860 - acc: 0.5467 - val_loss: 0.6913 - val_acc: 0.5229\n",
      "Epoch 22/100\n",
      "52900/52900 - 1s - loss: 0.6857 - acc: 0.5450 - val_loss: 0.6904 - val_acc: 0.5217\n",
      "Epoch 23/100\n",
      "52900/52900 - 1s - loss: 0.6855 - acc: 0.5470 - val_loss: 0.6916 - val_acc: 0.5214\n",
      "Epoch 24/100\n",
      "52900/52900 - 1s - loss: 0.6847 - acc: 0.5511 - val_loss: 0.6912 - val_acc: 0.5274\n",
      "Epoch 25/100\n",
      "52900/52900 - 1s - loss: 0.6840 - acc: 0.5510 - val_loss: 0.6915 - val_acc: 0.5250\n",
      "Epoch 26/100\n",
      "52900/52900 - 1s - loss: 0.6833 - acc: 0.5549 - val_loss: 0.6922 - val_acc: 0.5248\n",
      "Epoch 27/100\n",
      "52900/52900 - 1s - loss: 0.6830 - acc: 0.5567 - val_loss: 0.6926 - val_acc: 0.5196\n",
      "Epoch 28/100\n",
      "52900/52900 - 1s - loss: 0.6829 - acc: 0.5569 - val_loss: 0.6929 - val_acc: 0.5199\n",
      "Epoch 29/100\n",
      "52900/52900 - 1s - loss: 0.6821 - acc: 0.5580 - val_loss: 0.6942 - val_acc: 0.5181\n",
      "Epoch 30/100\n",
      "52900/52900 - 1s - loss: 0.6817 - acc: 0.5584 - val_loss: 0.6930 - val_acc: 0.5203\n",
      "Epoch 31/100\n",
      "52900/52900 - 1s - loss: 0.6811 - acc: 0.5604 - val_loss: 0.6932 - val_acc: 0.5183\n",
      "Epoch 32/100\n",
      "52900/52900 - 1s - loss: 0.6802 - acc: 0.5653 - val_loss: 0.6934 - val_acc: 0.5208\n",
      "Epoch 33/100\n",
      "52900/52900 - 1s - loss: 0.6807 - acc: 0.5619 - val_loss: 0.6940 - val_acc: 0.5183\n",
      "Epoch 34/100\n",
      "52900/52900 - 1s - loss: 0.6792 - acc: 0.5652 - val_loss: 0.6972 - val_acc: 0.5143\n",
      "Epoch 35/100\n",
      "52900/52900 - 1s - loss: 0.6793 - acc: 0.5652 - val_loss: 0.6952 - val_acc: 0.5229\n",
      "Epoch 36/100\n",
      "52900/52900 - 1s - loss: 0.6792 - acc: 0.5662 - val_loss: 0.6943 - val_acc: 0.5212\n",
      "Epoch 37/100\n",
      "52900/52900 - 1s - loss: 0.6780 - acc: 0.5693 - val_loss: 0.6961 - val_acc: 0.5172\n",
      "Epoch 00037: early stopping\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_4.h5 \n",
      "load scaler: tmp\\4-scaler.pkl\n",
      "valid_pred: [0.5318514  0.5097822  0.50722665 ... 0.507934   0.5244429  0.5074537 ]\n",
      "fold=5, acc=0.5283931947069943\n",
      "\n",
      "INFO: \n",
      "load model:tmp\\best_val_loss_4.h5 \n",
      "load scaler: tmp\\4-scaler.pkl\n",
      "test_pred: [0.4981122  0.5099525  0.50565004 ... 0.52734447 0.49227914 0.50256914] \n",
      "\n",
      "INFO: save csv tmp/submission_kernel.csv\n",
      "fold_mean_acc 0.5297088846880907\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_data(train, test, target_col):\n",
    "    \"\"\"前処理\"\"\"\n",
    "    df = train.copy()\n",
    "    df = df.append(test)\n",
    "    df = df.drop([\"id\", \"game-ver\"], axis=1)\n",
    "    \n",
    "    ## 時刻ばらす periodそのまま残すほうがcv acc上がる\n",
    "    #df[\"period\"] = pd.to_datetime(df[\"period\"])\n",
    "    #df['year'] = df[\"period\"].dt.year\n",
    "    #df['month'] = df[\"period\"].dt.month\n",
    "    #df['dayofyear'] = df[\"period\"].dt.dayofyear\n",
    "    #df['dayofweek'] = df[\"period\"].dt.dayofweek\n",
    "    #df['weekend'] = (df[\"period\"].dt.dayofweek.values >= 5).astype(int)\n",
    "    #df['hour'] = df[\"period\"].dt.hour\n",
    "    #df = df.drop([\"period\"], axis=1)\n",
    "    \n",
    "    # カテゴリ列保持\n",
    "    cat_features = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "    print(\"cat_features\", cat_features)\n",
    "    \n",
    "    # ラベルエンコディング\n",
    "    cate_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "    for col in cate_cols:\n",
    "        df[col], uni = pd.factorize(df[col])\n",
    "    \n",
    "    # 欠損置換（ラベルエンコディングの後じゃないと処理遅くなる）\n",
    "    nulls = df.drop(target_col, axis=1).isnull().sum().to_frame()\n",
    "    null_indexs = [index for index, row in nulls.iterrows() if row[0] > 0]\n",
    "    df = Encoder().impute_null_add_flag_col(df, cols_with_missing=null_indexs, strategy=\"most_frequent\")  # 最頻値で補間\n",
    "    \n",
    "    display(df.head(2))\n",
    "    \n",
    "    train = df[df[target_col].notnull()]  # 欠損ではない行のみ\n",
    "    test = df[df[target_col].isnull()]  # 欠損行のみ\n",
    "    test = test.drop(target_col, axis=1)\n",
    "    \n",
    "    return train, test, cat_features\n",
    "\n",
    "\n",
    "def enc_df(df_tr, df_va, df_te, target_col, cat_features):\n",
    "    \"\"\"\n",
    "    train/valid/testのtarget_encoderとか一括実行\n",
    "    エンコードで使うtrainのデータはvalid,test同じにしないとおかしくなるので\n",
    "    \"\"\"\n",
    "    print(\"before enc_df:\", df_tr.shape, df_va.shape, df_te.shape)\n",
    "    # エンコードの基準のデータフレーム\n",
    "    df_base = df_tr.copy()\n",
    "    \n",
    "    # train用\n",
    "    df_tr, df_va = Encoder().count_encoder(df_base, df_va, cat_features=cat_features)\n",
    "    #df_tr, df_va = Encoder().target_encoder(df_tr, df_va, target_col=target_col, cat_features=cat_features)\n",
    "    df_tr, df_va = Encoder().catboost_encoder(df_tr, df_va, target_col=target_col, cat_features=cat_features)\n",
    "    \n",
    "    # submit用\n",
    "    _df_tr, df_te = Encoder().count_encoder(df_base, df_te, cat_features=cat_features)\n",
    "    #_df_tr, df_te = Encoder().target_encoder(_df_tr, df_te, target_col=target_col, cat_features=cat_features)\n",
    "    _df_tr, df_te = Encoder().catboost_encoder(_df_tr, df_te, target_col=target_col, cat_features=cat_features)\n",
    "    \n",
    "    print(\"after enc_df:\", df_tr.shape, df_va.shape, df_te.shape)\n",
    "    return df_tr, df_va, df_te\n",
    "    \n",
    "\n",
    "def submit_cv_csv(test_preds, out_dir):\n",
    "    \"\"\"cvで出したtestの確信度のリストを平均してsubmit.csv出力\"\"\"\n",
    "    te_mean = pd.DataFrame(test_preds).apply(lambda x: np.mean(x), axis=1).values\n",
    "    te_mean[te_mean >= 0.5] = 1\n",
    "    te_mean[te_mean < 0.5] = 0\n",
    "    te_mean = te_mean.astype(int)\n",
    "    output_csv = f\"{out_dir}/submission_kernel.csv\"\n",
    "    pd.DataFrame({\"id\": range(len(te_mean)), \"y\": te_mean}).to_csv(\n",
    "        output_csv, index=False\n",
    "    )\n",
    "    print(f\"INFO: save csv {output_csv}\")\n",
    "    \n",
    "    \n",
    "def train_nn_cv(df_train, df_test, feats, target_col, params, encoder_flags, num_folds=2, cat_features=None):\n",
    "    \"\"\"cvでnnモデル学習してsubmit.csv出力\"\"\"\n",
    "    #folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    \n",
    "    test_preds = {}\n",
    "    fold_mean_acc = 0.0\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train[feats], df_train[target_col])):\n",
    "        print(\"n_fold:\", n_fold)\n",
    "    \n",
    "        # target_encoderとか\n",
    "        df_tr, df_va, df_te = enc_df(df_train.loc[train_idx], df_train.loc[valid_idx], df_test, target_col, cat_features)\n",
    "        train_x, train_y = df_tr[feats], df_tr[target_col]\n",
    "        valid_x, valid_y = df_va[feats], df_va[target_col]\n",
    "        test_x = df_te[feats]\n",
    "        \n",
    "        # model\n",
    "        model_cls = ModelNN(n_fold, params)\n",
    "        \n",
    "        # 学習\n",
    "        hist = model_cls.train(train_x, train_y, valid_x, valid_y)\n",
    "        \n",
    "        # 予測\n",
    "        valid_pred = model_cls.predict_binary(valid_x)\n",
    "        print(\"valid_pred:\", valid_pred)\n",
    "\n",
    "        # 正解率\n",
    "        valid_pred[valid_pred >= 0.5] = 1\n",
    "        valid_pred[valid_pred < 0.5] = 0\n",
    "        fold_acc = accuracy_score(valid_y, valid_pred)\n",
    "        fold_mean_acc += fold_acc / num_folds\n",
    "        print(f\"fold={num_folds}, acc={fold_acc}\\n\")\n",
    "        \n",
    "        # submitの確信度\n",
    "        test_pred = model_cls.predict_binary(test_x)\n",
    "        test_preds[n_fold] = test_pred\n",
    "        print(\"test_pred:\", test_pred, \"\\n\")\n",
    "        \n",
    "    # submit.csv出力\n",
    "    submit_cv_csv(test_preds, params[\"out_dir\"])\n",
    "    \n",
    "    return fold_mean_acc\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    target_col = \"y\"\n",
    "    df_train, df_test, cat_features = prepare_data(train, test, target_col)\n",
    "    #display(df_test)\n",
    "    \n",
    "    feats = df_test.columns.to_list()\n",
    "    params = dict(\n",
    "        out_dir=\"tmp\",\n",
    "        #scaler=MinMaxScaler(),\n",
    "        scaler=StandardScaler(),\n",
    "        layers=3,\n",
    "        units=[128, 64, 32],\n",
    "        dropout=[0.3, 0.3, 0.3],\n",
    "        nb_classes=2,\n",
    "        pred_activation=\"softmax\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate=0.001,\n",
    "        metrics=[\"acc\"],\n",
    "        #nb_epoch=10,\n",
    "        #patience=3,\n",
    "        nb_epoch=100,\n",
    "        patience=30,\n",
    "        batch_size=256,\n",
    "    )\n",
    "    num_folds = 5\n",
    "    \n",
    "    fold_mean_acc = train_nn_cv(df_train, df_test, feats, target_col, params, encoder_flags, num_folds=num_folds, cat_features=cat_features)\n",
    "    print(\"fold_mean_acc\", fold_mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
