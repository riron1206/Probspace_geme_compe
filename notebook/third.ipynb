{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T13:12:38.401570Z",
     "start_time": "2020-08-26T13:12:38.371651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/81908/jupyter_notebook/tf_2_work/Probspace_geme_compe/notebook\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T13:12:40.028438Z",
     "start_time": "2020-08-26T13:12:39.477740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\81908\\\\Anaconda3\\\\envs\\\\tfgpu\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 対戦ゲームデータ分析甲子園\n",
    "- https://prob.space/competitions/game_winner/data/62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T13:12:42.461231Z",
     "start_time": "2020-08-26T13:12:40.692672Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import joblib\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "import lightgbm as lgb\n",
    "from lightgbm import *\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\Git\\xfeat\")\n",
    "import xfeat\n",
    "from xfeat import *\n",
    "from xfeat.selector import *\n",
    "from xfeat.utils import compress_df\n",
    "\n",
    "sns.set()\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.set_option(\"display.max_columns\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T13:12:42.885604Z",
     "start_time": "2020-08-26T13:12:42.462226Z"
    },
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1-level</th>\n",
       "      <th>A1-rank</th>\n",
       "      <th>A1-weapon</th>\n",
       "      <th>A2-level</th>\n",
       "      <th>A2-rank</th>\n",
       "      <th>A2-weapon</th>\n",
       "      <th>A3-level</th>\n",
       "      <th>A3-rank</th>\n",
       "      <th>A3-weapon</th>\n",
       "      <th>A4-level</th>\n",
       "      <th>A4-rank</th>\n",
       "      <th>A4-weapon</th>\n",
       "      <th>B1-level</th>\n",
       "      <th>B1-rank</th>\n",
       "      <th>B1-weapon</th>\n",
       "      <th>B2-level</th>\n",
       "      <th>B2-rank</th>\n",
       "      <th>B2-weapon</th>\n",
       "      <th>B3-level</th>\n",
       "      <th>B3-rank</th>\n",
       "      <th>B3-weapon</th>\n",
       "      <th>B4-level</th>\n",
       "      <th>B4-rank</th>\n",
       "      <th>B4-weapon</th>\n",
       "      <th>game-ver</th>\n",
       "      <th>id</th>\n",
       "      <th>lobby</th>\n",
       "      <th>lobby-mode</th>\n",
       "      <th>mode</th>\n",
       "      <th>period</th>\n",
       "      <th>stage</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sshooter_becchu</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soytuber_custom</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pablo_hue</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hokusai</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bold_7</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hokusai_becchu</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>herocharger_replica</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sharp_neo</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>standard</td>\n",
       "      <td>regular</td>\n",
       "      <td>nawabari</td>\n",
       "      <td>2019-10-15T20:00:00+00:00</td>\n",
       "      <td>sumeshi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parashelter_sorella</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jetsweeper</td>\n",
       "      <td>198.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>campingshelter_camo</td>\n",
       "      <td>123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nzap85</td>\n",
       "      <td>83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>momiji</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>squiclean_b</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>campingshelter</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sputtery_clear</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>standard</td>\n",
       "      <td>regular</td>\n",
       "      <td>nawabari</td>\n",
       "      <td>2019-12-14T04:00:00+00:00</td>\n",
       "      <td>arowana</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>a-</td>\n",
       "      <td>nzap89</td>\n",
       "      <td>68.0</td>\n",
       "      <td>a</td>\n",
       "      <td>quadhopper_black</td>\n",
       "      <td>225.0</td>\n",
       "      <td>a</td>\n",
       "      <td>prime_becchu</td>\n",
       "      <td>107.0</td>\n",
       "      <td>a</td>\n",
       "      <td>jetsweeper</td>\n",
       "      <td>50</td>\n",
       "      <td>a-</td>\n",
       "      <td>bold_7</td>\n",
       "      <td>163.0</td>\n",
       "      <td>a+</td>\n",
       "      <td>nzap85</td>\n",
       "      <td>160.0</td>\n",
       "      <td>a-</td>\n",
       "      <td>prime_becchu</td>\n",
       "      <td>126.0</td>\n",
       "      <td>a</td>\n",
       "      <td>dualsweeper_custom</td>\n",
       "      <td>5.0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>standard</td>\n",
       "      <td>gachi</td>\n",
       "      <td>hoko</td>\n",
       "      <td>2019-12-25T14:00:00+00:00</td>\n",
       "      <td>ama</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1-level A1-rank            A1-weapon  A2-level A2-rank         A2-weapon  \\\n",
       "0       139     NaN      sshooter_becchu     118.0     NaN   soytuber_custom   \n",
       "1       198     NaN  parashelter_sorella      77.0     NaN        jetsweeper   \n",
       "2       114      a-               nzap89      68.0       a  quadhopper_black   \n",
       "\n",
       "   A3-level A3-rank            A3-weapon  A4-level A4-rank   A4-weapon  \\\n",
       "0      13.0     NaN            pablo_hue      10.0     NaN     hokusai   \n",
       "1     198.0     NaN  campingshelter_camo     123.0     NaN      nzap85   \n",
       "2     225.0       a         prime_becchu     107.0       a  jetsweeper   \n",
       "\n",
       "   B1-level B1-rank B1-weapon  B2-level B2-rank       B2-weapon  B3-level  \\\n",
       "0        28     NaN    bold_7      26.0     NaN  hokusai_becchu      68.0   \n",
       "1        83     NaN    momiji     118.0     NaN     squiclean_b     168.0   \n",
       "2        50      a-    bold_7     163.0      a+          nzap85     160.0   \n",
       "\n",
       "  B3-rank            B3-weapon  B4-level B4-rank           B4-weapon game-ver  \\\n",
       "0     NaN  herocharger_replica      31.0     NaN           sharp_neo    5.0.1   \n",
       "1     NaN       campingshelter     151.0     NaN      sputtery_clear    5.0.1   \n",
       "2      a-         prime_becchu     126.0       a  dualsweeper_custom    5.0.1   \n",
       "\n",
       "   id     lobby lobby-mode      mode                     period    stage    y  \n",
       "0   1  standard    regular  nawabari  2019-10-15T20:00:00+00:00  sumeshi  1.0  \n",
       "1   2  standard    regular  nawabari  2019-12-14T04:00:00+00:00  arowana  0.0  \n",
       "2   3  standard      gachi      hoko  2019-12-25T14:00:00+00:00      ama  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUT_DATA = r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\\Probspace_geme_compe\\data\\third\"\n",
    "os.makedirs(OUT_DATA, exist_ok=True)\n",
    "\n",
    "OUT_MODEL = (\n",
    "    r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\\Probspace_geme_compe\\model\\third\"\n",
    ")\n",
    "os.makedirs(OUT_MODEL, exist_ok=True)\n",
    "\n",
    "WORK_DIR = r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\\Probspace_geme_compe\\data\\work\"\n",
    "\n",
    "ORIG = r\"C:\\Users\\81908\\jupyter_notebook\\tf_2_work\\Probspace_geme_compe\\data\\orig\"\n",
    "train_df = pd.read_csv(f\"{ORIG}/train_data.csv\")\n",
    "test_df = pd.read_csv(f\"{ORIG}/test_data.csv\")\n",
    "df_all = train_df.append(test_df).reset_index(drop=True)\n",
    "display(df_all.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "helper関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T13:12:47.243516Z",
     "start_time": "2020-08-26T13:12:47.172708Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_corr(df, target_col=\"y\", png_path=None):\n",
    "    \"\"\"目的変数との数値列との相関係数確認\"\"\"\n",
    "    num_cols = df.select_dtypes(\n",
    "        include=[\"int\", \"int32\", \"int64\", \"float\", \"float32\", \"float64\"]\n",
    "    ).columns.to_list()\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "    corrs = []\n",
    "    for col in num_cols:\n",
    "        s1 = df[col]\n",
    "        s2 = df[target_col]\n",
    "        corr = s1.corr(s2)\n",
    "        corrs.append(abs(round(corr, 3)))\n",
    "\n",
    "    df_corr = pd.DataFrame({\"feature\": num_cols, \"y_corr\": corrs}).sort_values(\n",
    "        by=\"y_corr\", ascending=False\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(\n",
    "        x=\"y_corr\", y=\"feature\", data=df_corr.head(50),\n",
    "    )\n",
    "    plt.title(f\"target_corr\")\n",
    "    plt.tight_layout()\n",
    "    if png_path is not None:\n",
    "        plt.savefig(png_path)\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T17:32:40.712389Z",
     "start_time": "2020-08-25T17:27:18.738750Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(f\"{WORK_DIR}/eda_preprocess.csv\", index_col=0)\n",
    "\n",
    "# label_encoding\n",
    "cate_cols = df_all.select_dtypes(\n",
    "    include=[\"object\", \"category\", \"bool\"]\n",
    ").columns.to_list()\n",
    "for col in cate_cols:\n",
    "    df_all[col], uni = pd.factorize(df_all[col])\n",
    "\n",
    "# ファイル出力\n",
    "# df_all.to_csv(f\"{OUT_DATA}/label_encoding.csv\", index=False)\n",
    "# display(df_all)\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T21:31:24.655105Z",
     "start_time": "2020-08-25T17:32:40.713359Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_useless_features(df, cols=None, threshold=0.8):\n",
    "    \"\"\"\n",
    "    xfeatで不要な特徴量削除\n",
    "    - 列の内容が重複している列削除\n",
    "    - すべて同じ値の列削除\n",
    "    - スピマンの相関係数が高い列（多重共変性ある列）削除.相関係数がthresholdより高い列が消される\n",
    "    https://github.com/pfnet-research/xfeat/blob/master/examples/remove_useless_features.py\n",
    "    \"\"\"\n",
    "    # データ型を変換してメモリ使用量を削減\n",
    "    cols = df.columns.tolist() if cols is None else cols\n",
    "    df = compress_df(pd.DataFrame(data=df, columns=cols))\n",
    "\n",
    "    encoder = Pipeline(\n",
    "        [\n",
    "            DuplicatedFeatureEliminator(),\n",
    "            ConstantFeatureEliminator(),\n",
    "            # SpearmanCorrelationEliminator(threshold=threshold),  # 相関係数>thresholdの特長削除\n",
    "        ]\n",
    "    )\n",
    "    df_reduced = encoder.fit_transform(df)\n",
    "    # print(\"Selected columns: {}\".format(df_reduced.columns.tolist()))\n",
    "    return df_reduced\n",
    "\n",
    "\n",
    "y_seri = df_all[\"y\"]\n",
    "df_all = df_all.drop(\"y\", axis=1)\n",
    "df_all = remove_useless_features(df_all)\n",
    "df_all = pd.concat([df_all, y_seri], axis=1)\n",
    "# display(df_all)\n",
    "df_all.to_csv(f\"{OUT_DATA}/remove_useless_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T21:34:11.273904Z",
     "start_time": "2020-08-25T21:31:24.659070Z"
    },
    "code_folding": [
     2
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_feature_selection(\n",
    "    df, target_col, params,\n",
    "):\n",
    "    \"\"\"feature_importanceの閾値固定して特徴量選択\"\"\"\n",
    "    # 特徴量の列名（取捨選択前）\n",
    "    input_cols = df.columns.tolist()\n",
    "    n_before_selection = len(input_cols)\n",
    "    input_cols.remove(target_col)\n",
    "\n",
    "    # 特徴量選択用モデル取得\n",
    "    lgbm_params = {\n",
    "        \"objective\": params[\"objective\"],\n",
    "        \"metric\": params[\"metric\"],\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "    selector = GBDTFeatureSelector(\n",
    "        input_cols=input_cols,\n",
    "        target_col=target_col,\n",
    "        threshold=params[\"threshold\"],\n",
    "        lgbm_params=lgbm_params,\n",
    "    )\n",
    "    selector.fit(df)\n",
    "\n",
    "    # 選択をした特徴量を返す\n",
    "    selected_cols = selector.get_selected_cols()\n",
    "    print(f\" - {n_before_selection - len(selected_cols)} features are removed.\")\n",
    "    return df[selected_cols]\n",
    "\n",
    "\n",
    "target_col = \"y\"\n",
    "# metric=roc_aucでも可能\n",
    "# feature_importance高い順に列数を 列数*threshold にする\n",
    "n_cols = 600\n",
    "threshold = n_cols / df_all.shape[1]\n",
    "params = {\"metric\": \"binary_logloss\", \"objective\": \"binary\", \"threshold\": threshold}\n",
    "df_all = run_feature_selection(df_all, target_col, params)\n",
    "print(df_all.shape)\n",
    "print(df_all.columns)\n",
    "\n",
    "# 列名保持\n",
    "feature_selections = sorted(df_all.columns.to_list())\n",
    "pd.DataFrame({\"feature_selections\": feature_selections}).to_csv(\n",
    "    f\"{OUT_DATA}/feature_selections.csv\", index=False\n",
    ")\n",
    "\n",
    "# ファイル出力\n",
    "df_all.to_csv(f\"{OUT_DATA}/run_feature_selection.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T07:07:51.727484Z",
     "start_time": "2020-08-26T06:14:25.404911Z"
    },
    "code_folding": [],
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_rfecv(selector):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set()\n",
    "\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "df_all = pd.read_csv(f\"{OUT_DATA}/feature_add.csv\")\n",
    "cols = pd.read_csv(f\"{OUT_DATA}/feature_selections.csv\")[\"feature_selections\"].values.tolist()\n",
    "target_col = \"y\"\n",
    "cols.append(target_col)\n",
    "df_all = df_all[cols]   \n",
    "print(df_all.shape)\n",
    "    \n",
    "X_train = df_all.loc[df_all[\"y\"].notnull()]\n",
    "cate_cols = X_train.select_dtypes(\n",
    "    include=[\"object\", \"category\", \"bool\"]\n",
    ").columns.to_list()\n",
    "for col in cate_cols:\n",
    "    X_train[col], uni = pd.factorize(X_train[col])\n",
    "y_train = X_train[\"y\"]\n",
    "\n",
    "best_params = {'bagging_fraction': 0.9, 'bagging_freq': 6, 'feature_fraction': 0.1, 'max_depth': 7, 'min_child_samples': 343, 'min_child_weight': 0.04084861948055769, 'num_leaves': 95, 'reg_alpha': 0.5612212694825488, 'reg_lambda': 0.0001757886119766502}\n",
    "clf = lgb.LGBMClassifier(n_jobs=-1, seed=71, **best_params)  # 欠損ある場合はGBM使う（リッジより遅い）\n",
    "\n",
    "# RFECVは交差検証+再帰的特徴除去。データでかいとメモリ死ぬので注意\n",
    "# RFE（再帰的特徴除去=recursive feature elimination: すべての特徴量を使う状態から、1つずつ特徴量を取り除いていく）で特徴量選択\n",
    "selector = RFECV(clf, cv=KFold(3, shuffle=True), scoring=\"accuracy\", n_jobs=-1)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 選択した特徴量\n",
    "select_cols = X_train.columns[selector.get_support()].to_list()\n",
    "print(\"\\nselect_cols:\\n\", select_cols)\n",
    "# 捨てた特徴量\n",
    "print(\"not select_cols:\\n\", X_train.columns[~selector.get_support()].to_list())\n",
    "plot_rfecv(selector)\n",
    "df_all.to_csv(f\"{OUT_DATA}/rfecv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:53:06.815870Z",
     "start_time": "2020-08-26T08:53:06.739766Z"
    },
    "code_folding": [
     0,
     28,
     55
    ]
   },
   "outputs": [],
   "source": [
    "def count_encoder(train_df, valid_df, cat_features=None):\n",
    "    \"\"\"\n",
    "    Count_Encoding: カテゴリ列をカウント値に変換する特徴量エンジニアリング（要はgroupby().size()の集計列追加のこと）\n",
    "    ※カウント数が同じカテゴリは同じようなデータ傾向になる可能性がある\n",
    "    https://www.kaggle.com/matleonard/categorical-encodings\n",
    "    \"\"\"\n",
    "    # conda install -c conda-forge category_encoders\n",
    "    import category_encoders as ce\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = train_df.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"bool\"]\n",
    "        ).columns.to_list()\n",
    "\n",
    "    count_enc = ce.CountEncoder(cols=cat_features)\n",
    "\n",
    "    # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "    count_enc.fit(train_df[cat_features])\n",
    "    train_encoded = train_df.join(\n",
    "        count_enc.transform(train_df[cat_features]).add_suffix(\"_count\")\n",
    "    )\n",
    "    valid_encoded = valid_df.join(\n",
    "        count_enc.transform(valid_df[cat_features]).add_suffix(\"_count\")\n",
    "    )\n",
    "\n",
    "    return train_encoded, valid_encoded\n",
    "\n",
    "\n",
    "def target_encoder(train_df, valid_df, target_col: str, cat_features=None):\n",
    "    \"\"\"\n",
    "    Target_Encoding: カテゴリ列を目的変数の平均値に変換する特徴量エンジニアリング\n",
    "    https://www.kaggle.com/matleonard/categorical-encodings\n",
    "    \"\"\"\n",
    "    # conda install -c conda-forge category_encoders\n",
    "    import category_encoders as ce\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = train_df.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"bool\"]\n",
    "        ).columns.to_list()\n",
    "\n",
    "    target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "    # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "    target_enc.fit(train_df[cat_features], train_df[target_col])\n",
    "\n",
    "    train_encoded = train_df.join(\n",
    "        target_enc.transform(train_df[cat_features]).add_suffix(\"_target\")\n",
    "    )\n",
    "    valid_encoded = valid_df.join(\n",
    "        target_enc.transform(valid_df[cat_features]).add_suffix(\"_target\")\n",
    "    )\n",
    "    return train_encoded, valid_encoded\n",
    "\n",
    "\n",
    "def catboost_encoder(train_df, valid_df, target_col: str, cat_features=None):\n",
    "    \"\"\"\n",
    "    CatBoost_Encoding: カテゴリ列を目的変数の1行前の行からのみに変換する特徴量エンジニアリング\n",
    "    CatBoost使ったターゲットエンコーディング\n",
    "    https://www.kaggle.com/matleonard/categorical-encodings\n",
    "    \"\"\"\n",
    "    # conda install -c conda-forge category_encoders\n",
    "    import category_encoders as ce\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = train_df.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"bool\"]\n",
    "        ).columns.to_list()\n",
    "\n",
    "    cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n",
    "\n",
    "    # trainだけでfitすること(validationやtest含めるとリークする)\n",
    "    cb_enc.fit(train_df[cat_features], train_df[target_col])\n",
    "\n",
    "    train_encoded = train_df.join(\n",
    "        cb_enc.transform(train_df[cat_features]).add_suffix(\"_cb\")\n",
    "    )\n",
    "    valid_encoded = valid_df.join(\n",
    "        cb_enc.transform(valid_df[cat_features]).add_suffix(\"_cb\")\n",
    "    )\n",
    "    return train_encoded, valid_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:00:19.841145Z",
     "start_time": "2020-08-26T08:53:46.145489Z"
    },
    "code_folding": [
     0,
     166,
     179
    ],
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (66125, 128), test shape: (28340, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run encoding Train shape: (49593, 476), valid shape: (16532, 476)\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_error: 0.104733\ttraining's binary_logloss: 0.413727\tvalid_1's binary_error: 0.482579\tvalid_1's binary_logloss: 0.731293\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's binary_error: 0.288146\ttraining's binary_logloss: 0.656498\tvalid_1's binary_error: 0.47399\tvalid_1's binary_logloss: 0.690825\n",
      "Fold  1 error : 0.473990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [03:13, 193.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run encoding Train shape: (49594, 476), valid shape: (16531, 476)\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_error: 0.104569\ttraining's binary_logloss: 0.414881\tvalid_1's binary_error: 0.48769\tvalid_1's binary_logloss: 0.735605\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's binary_error: 0.360124\ttraining's binary_logloss: 0.663423\tvalid_1's binary_error: 0.478192\tvalid_1's binary_logloss: 0.691521\n",
      "Fold  2 error : 0.478192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [06:26, 193.52s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5782a82ea9df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mis_plot_perm_importance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mfeat_importance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkfold_cv_LGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-5782a82ea9df>\u001b[0m in \u001b[0;36mkfold_cv_LGBMClassifier\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# カウントエンコディング\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             t_fold_df, v_fold_df = count_encoder(\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mt_fold_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_fold_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# ターゲットエンコディング\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-5e278d17a706>\u001b[0m in \u001b[0;36mcount_encoder\u001b[1;34m(train_df, valid_df, cat_features)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcount_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     train_encoded = train_df.join(\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mcount_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     )\n\u001b[0;32m     22\u001b[0m     valid_encoded = valid_df.join(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\category_encoders\\count.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_count_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_invariant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\category_encoders\\count.py\u001b[0m in \u001b[0;36m_transform_count_encode\u001b[1;34m(self, X_in, y)\u001b[0m\n\u001b[0;32m    263\u001b[0m                     )\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_unknown\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3486\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3487\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3564\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3565\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3567\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3382\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3384\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self, OUTPUT_DIR):\n",
    "        self.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "    # LightGBM GBDT with KFold or Stratified KFold\n",
    "    # Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "    def kfold_cv_LGBMClassifier(\n",
    "        self,\n",
    "        lgb_params: dict,\n",
    "        df: pd.DataFrame,\n",
    "        num_folds: int,\n",
    "        target_col: str,\n",
    "        del_cols=None,\n",
    "        eval_metric=\"error\",\n",
    "        stratified=True,  # StratifiedKFoldにするか\n",
    "        is_submission=False,  # Home_Credit_Default_Risk の submission.csv作成するか\n",
    "        is_plot_perm_importance=False,  # permutation importanceも出すか. feature_importance はデフォルトでだす\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LGBMClassifierでcross validation + feature_importance/permutation importance plot\n",
    "        \"\"\"\n",
    "        # Divide in training/validation and test data\n",
    "        train_df = df[df[target_col].notnull()].reset_index(drop=True)\n",
    "        test_df = df[df[target_col].isnull()].reset_index(drop=True)\n",
    "        print(\n",
    "            \"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "                train_df.shape, test_df.shape\n",
    "            )\n",
    "        )\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        # Cross validation model\n",
    "        if stratified:\n",
    "            folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "        else:\n",
    "            folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "        # Create arrays and dataframes to store results\n",
    "        oof_preds = np.zeros(train_df.shape[0])\n",
    "        sub_preds = np.zeros(test_df.shape[0])\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        permutation_importance_df = pd.DataFrame()\n",
    "        result_scores = {}\n",
    "        train_probas = {}\n",
    "        test_probas = {}\n",
    "\n",
    "        # 目的変数とID列など削除\n",
    "        del_cols = del_cols.append(target_col) if del_cols is not None else [target_col]\n",
    "        feats = [f for f in train_df.columns if f not in del_cols]\n",
    "\n",
    "        for n_fold, (train_idx, valid_idx) in tqdm(\n",
    "            enumerate(folds.split(train_df[feats], train_df[target_col]))\n",
    "        ):\n",
    "            t_fold_df = train_df.iloc[train_idx]\n",
    "            v_fold_df = train_df.iloc[valid_idx]\n",
    "\n",
    "            # カウントエンコディング\n",
    "            t_fold_df, v_fold_df = count_encoder(\n",
    "                t_fold_df, v_fold_df, cat_features=None\n",
    "            )\n",
    "            # ターゲットエンコディング\n",
    "            t_fold_df, v_fold_df = target_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # CatBoostエンコディング\n",
    "            t_fold_df, v_fold_df = catboost_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # ラベルエンコディング\n",
    "            cate_cols = t_fold_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                t_fold_df[col], uni = pd.factorize(t_fold_df[col])\n",
    "                v_fold_df[col], uni = pd.factorize(v_fold_df[col])\n",
    "            print(\n",
    "                \"run encoding Train shape: {}, valid shape: {}\".format(\n",
    "                    t_fold_df.shape, v_fold_df.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "            feats = t_fold_df.columns.to_list()\n",
    "            feats.remove(target_col)\n",
    "\n",
    "            train_x, train_y = (\n",
    "                t_fold_df[feats],\n",
    "                t_fold_df[target_col],\n",
    "            )\n",
    "            valid_x, valid_y = (\n",
    "                v_fold_df[feats],\n",
    "                v_fold_df[target_col],\n",
    "            )\n",
    "\n",
    "            ############################ train fit ############################\n",
    "            # LightGBM parameters found by Bayesian optimization\n",
    "            clf = LGBMClassifier(**lgb_params)\n",
    "            clf.fit(\n",
    "                train_x,\n",
    "                train_y,\n",
    "                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                eval_metric=eval_metric,\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200,\n",
    "            )\n",
    "\n",
    "            # モデル保存\n",
    "            joblib.dump(clf, f\"{self.OUTPUT_DIR}/lgb-{n_fold + 1}.model\", compress=True)\n",
    "\n",
    "            # valid pred\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x, num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "\n",
    "            ############################ test pred ############################\n",
    "            # カウントエンコディング\n",
    "            tr_df, te_df = count_encoder(train_df, test_df, cat_features=None)\n",
    "            # ターゲットエンコディング\n",
    "            tr_df, te_df = target_encoder(\n",
    "                tr_df, te_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # CatBoostエンコディング\n",
    "            tr_df, te_df = catboost_encoder(\n",
    "                tr_df, te_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # ラベルエンコディング\n",
    "            cate_cols = tr_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                tr_df[col], uni = pd.factorize(tr_df[col])\n",
    "                te_df[col], uni = pd.factorize(te_df[col])\n",
    "                \n",
    "            # testの確信度\n",
    "            test_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(\n",
    "                te_df[feats], num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "            sub_preds += test_probas[f\"fold_{str(n_fold + 1)}\"] / folds.n_splits\n",
    "\n",
    "            # 一応trainの確信度も出しておく\n",
    "            train_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(\n",
    "                tr_df[feats], num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "\n",
    "            if eval_metric == \"auc\":\n",
    "                fold_auc = roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "                print(\"Fold %2d AUC : %.6f\" % (n_fold + 1, fold_auc))\n",
    "                result_scores[f\"fold_auc_{str(n_fold + 1)}\"] = fold_auc\n",
    "            elif eval_metric == \"error\":\n",
    "                # intにしないとaccuracy_score()エラーになる\n",
    "                _pred = oof_preds[valid_idx]\n",
    "                _pred[_pred >= 0.5] = 1\n",
    "                _pred[_pred < 0.5] = 0\n",
    "                fold_err = 1.0 - accuracy_score(valid_y, _pred)\n",
    "                print(\"Fold %2d error : %.6f\" % (n_fold + 1, fold_err))\n",
    "                result_scores[f\"fold_err_{str(n_fold + 1)}\"] = fold_err\n",
    "\n",
    "            # feature_importance\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat(\n",
    "                [feature_importance_df, fold_importance_df], axis=0\n",
    "            )\n",
    "\n",
    "            if is_plot_perm_importance:\n",
    "                # permutation_importance\n",
    "                # 時間かかるからifで制御する\n",
    "                # scoringはsklearnのスコアリングパラメータ\n",
    "                # accuracy や neg_mean_squared_log_error とか\n",
    "                # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                fold_importance_df = pd.DataFrame()\n",
    "                fold_permutation = permutation_importance(\n",
    "                    clf, valid_x, valid_y, scoring=\"roc_auc\"\n",
    "                )\n",
    "                fold_permutation_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"feature\": valid_x.columns,\n",
    "                        \"importance\": np.abs(\n",
    "                            fold_permutation[\"importances_mean\"]\n",
    "                        ),  # マイナスとるのもあるので絶対値にする\n",
    "                        \"fold\": n_fold + 1,\n",
    "                    },\n",
    "                )\n",
    "                permutation_importance_df = pd.concat(\n",
    "                    [permutation_importance_df, fold_permutation_df], axis=0\n",
    "                )\n",
    "\n",
    "            del clf, train_x, train_y, valid_x, valid_y\n",
    "            gc.collect()\n",
    "\n",
    "        if eval_metric == \"auc\":\n",
    "            mean_fold_auc = roc_auc_score(train_df[target_col], oof_preds)\n",
    "            print(\"Full AUC score %.6f\" % mean_fold_auc)\n",
    "            result_scores[\"mean_fold_auc\"] = mean_fold_auc\n",
    "        elif eval_metric == \"error\":\n",
    "            # intにしないとaccuracy_score()エラーになる\n",
    "            _pred = oof_preds\n",
    "            _pred[_pred >= 0.5] = 1\n",
    "            _pred[_pred < 0.5] = 0\n",
    "            mean_fold_err = 1.0 - accuracy_score(train_df[target_col], _pred)\n",
    "            print(\"Full error score %.6f\" % mean_fold_err)\n",
    "            result_scores[\"mean_fold_err\"] = mean_fold_err\n",
    "\n",
    "        # モデルのスコア出力\n",
    "        result_scores_df = pd.DataFrame(\n",
    "            result_scores.values(), index=result_scores.keys()\n",
    "        )\n",
    "        result_scores_df.to_csv(f\"{self.OUTPUT_DIR}/result_scores.tsv\", sep=\"\\t\")\n",
    "\n",
    "        test_probas_df = pd.DataFrame(test_probas)\n",
    "        test_probas_df.to_csv(f\"{self.OUTPUT_DIR}/test_probas.tsv\", index=False)\n",
    "        train_probas_df = pd.DataFrame(train_probas)\n",
    "        train_probas_df.to_csv(f\"{self.OUTPUT_DIR}/train_probas.tsv\", index=False)\n",
    "\n",
    "        # Write submission file (Home_Credit_Default_Risk)\n",
    "        if is_submission:\n",
    "            sub_preds[sub_preds >= 0.5] = 1\n",
    "            sub_preds[sub_preds < 0.5] = 0\n",
    "            test_df[target_col] = sub_preds\n",
    "            submission_file_name = f\"{self.OUTPUT_DIR}/submission_kernel.csv\"\n",
    "            sub_df = test_df[[target_col]]\n",
    "            sub_df[\"id\"] = test_df.index\n",
    "            sub_df.astype(int)\n",
    "            sub_df = sub_df[[\"id\", \"y\"]]\n",
    "            sub_df.to_csv(submission_file_name, index=False)\n",
    "\n",
    "        # Plot feature importance\n",
    "        Model(\"\").display_importances(\n",
    "            feature_importance_df,\n",
    "            png_path=f\"{self.OUTPUT_DIR}/lgbm_feature_importances.png\",\n",
    "            title=\"feature_importance\",\n",
    "        )\n",
    "        if is_plot_perm_importance:\n",
    "            Model(\"\").display_importances(\n",
    "                permutation_importance_df,\n",
    "                png_path=f\"{self.OUTPUT_DIR}/lgbm_permutation_importances.png\",\n",
    "                title=\"permutation_importance\",\n",
    "            )\n",
    "\n",
    "        return feature_importance_df, permutation_importance_df\n",
    "\n",
    "    # Display/plot feature/permutation importance\n",
    "    @staticmethod\n",
    "    def display_importances(\n",
    "        importance_df_, png_path, title,\n",
    "    ):\n",
    "        cols = (\n",
    "            importance_df_[[\"feature\", \"importance\"]]\n",
    "            .groupby(\"feature\")\n",
    "            .mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:40]\n",
    "            .index\n",
    "        )\n",
    "        best_features = importance_df_.loc[importance_df_.feature.isin(cols)]\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        sns.barplot(\n",
    "            x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False),\n",
    "        )\n",
    "        plt.title(f\"LightGBM {title} (avg over folds)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_all = pd.read_csv(f\"{WORK_DIR}/eda_preprocess.csv\", index_col=0)\n",
    "    #cols = pd.read_csv(f\"{OUT_DATA}/feature_selections.csv\")[\"feature_selections\"].values.tolist()\n",
    "    #cols.append(\"y\")\n",
    "    #cols = ['A1-levelA2-levelA3-levelA4-level_mul', 'A1-rank_A1-weapon_A1-level_diff_mean_x', 'A1-rank_A1-weapon_A1-level_mean_var_x', 'A1-rank_A1-weapon_A1-level_ratio_range_x', 'A1-rank_A1-weapon_A2-level_beyond1std_x', 'A1-rank_A1-weapon_A2-level_kurt_x', 'A1-rank_A1-weapon_A2-level_sem_x', 'A1-rank_A1-weapon_A2-level_skew_x', 'A1-rank_A1-weapon_A2-level_var_x', 'A1-rank_A1-weapon_A3-level_sem_x', 'A1-rank_A1-weapon_A4-level_hl_ratio_x', 'A1-rank_A1-weapon_A4-level_percentile_75_x', 'A1-rank_A1-weapon_B1-level_hl_ratio_x', 'A1-rank_A1-weapon_B1-level_var_x', 'A1-rank_A1-weapon_B3-level_kurt_x', 'A1-rank_A1-weapon_B3-level_var_x', 'A1-rank_A1-weapon_B4-level_kurt_x', 'A1-rank_A1-weapon_B4-level_zscore_x', 'A1-weapon_A2-level_zscore_x', 'A1-weapon_B1-level_beyond1std_x', 'A1-weapon_B4-level_max_x', 'A2-rank_A2-weapon_A1-level_mad', 'A2-rank_A2-weapon_A2-level_mean', 'A2-rank_A2-weapon_A4-level_kurt', 'A2-rank_A2-weapon_A4-level_skew', 'A2-rank_A2-weapon_B2-level_max', 'A2-rank_A2-weapon_B2-level_skew', 'A2-rank_A2-weapon_B2-level_zscore', 'A2-rank_A2-weapon_B3-level_kurt', 'A2-rank_A2-weapon_B4-level_diff_mean', 'A2-rank_A2-weapon_B4-level_diff_percentile_75-25', 'A2-rank_A2-weapon_B4-level_percentile_25', 'A2-weapon_A3-level_ptp_x', 'A2-weapon_A4-level_diff_percentile_75-25_x', 'A2-weapon_B3-level_percentile_75_x', 'A3-rank_A3-weapon_A1-level_sem', 'A3-rank_A3-weapon_A2-level_hl_ratio', 'A3-rank_A3-weapon_A2-level_kurt', 'A3-rank_A3-weapon_A2-level_ratio_range', 'A3-rank_A3-weapon_A3-level_kurt', 'A3-rank_A3-weapon_A4-level_ratio_range', 'A3-rank_A3-weapon_B1-level_diff_mean', 'A3-rank_A3-weapon_B1-level_zscore', 'A3-rank_A3-weapon_B2-level_hl_ratio', 'A3-rank_A3-weapon_B2-level_mean', 'A3-rank_A3-weapon_B2-level_var', 'A3-rank_A3-weapon_B3-level_ratio_mean', 'A3-rank_A3-weapon_B4-level_kurt', 'A3-rank_A3-weapon_B4-level_percentile_25', 'A3-weapon_A1-level_kurt_x', 'A3-weapon_A2-level_skew_x', 'A3-weapon_A3-level_hl_ratio_x', 'A3-weapon_A3-level_max_x', 'A3-weapon_A4-level_percentile_75_x', 'A3-weapon_B3-level_max_x', 'A4-rank_A4-weapon', 'A4-rank_A4-weapon_A1-level_kurt', 'A4-rank_A4-weapon_A3-level_ratio_range', 'A4-rank_A4-weapon_A4-level_diff_mean', 'A4-rank_A4-weapon_A4-level_diff_percentile_75-25', 'A4-rank_A4-weapon_A4-level_ratio_range', 'A4-rank_A4-weapon_B2-level_ratio_range', 'A4-weapon_A1-level_diff_percentile_75-25_x', 'A4-weapon_A1-level_hl_ratio_x', 'A4-weapon_A4-level_mad_x', 'A4-weapon_A4-level_median_x', 'A4-weapon_A4-level_percentile_75_x', 'A4-weapon_A4-level_var_x', 'A4-weapon_B4-level_mad_x', 'A4-weapon_B4-level_mean_var_x', 'A_ranks_A2-level_kurt', 'A_ranks_weapons_B2-level_percentile_25', 'A_weapons_A1-level_min', 'A_weapons_A1-level_prod', 'A_weapons_A2-level_prod', 'A_weapons_A3-level_prod', 'A_weapons_A3-level_sum', 'A_weapons_B1-level_max', 'A_weapons_B1-level_percentile_25', 'A_weapons_B3-level_min', 'A_weapons_B3-level_percentile_75', 'A_weapons_B3-level_prod', 'A_weapons_B4-level_mean', 'B1-levelB2-levelB3-levelB4-level_plus', 'B1-rank_B1-weapon_A1-level_kurt_x', 'B1-rank_B1-weapon_A1-level_var_x', 'B1-rank_B1-weapon_A2-level_kurt_x', 'B1-rank_B1-weapon_A2-level_ptp_x', 'B1-rank_B1-weapon_A2-level_sem_x', 'B1-rank_B1-weapon_A2-level_var_x', 'B1-rank_B1-weapon_A3-level_skew_x', 'B1-rank_B1-weapon_A4-level_skew_x', 'B1-rank_B1-weapon_B1-level_sem_x', 'B1-rank_B1-weapon_B4-level_kurt_x', 'B1-rank_B1-weapon_B4-level_ptp_x', 'B1-weapon', 'B1-weapon_A2-level_percentile_25_x', 'B1-weapon_A2-level_skew_x', 'B2-rank_B2-weapon_A1-level_diff_percentile_75-25', 'B2-rank_B2-weapon_A1-level_ratio_range', 'B2-rank_B2-weapon_A1-level_sem', 'B2-rank_B2-weapon_A4-level_diff_percentile_75-25', 'B2-rank_B2-weapon_A4-level_ptp', 'B2-rank_B2-weapon_A4-level_sem', 'B2-rank_B2-weapon_B1-level_kurt', 'B2-rank_B2-weapon_B2-level_max', 'B2-rank_B2-weapon_B2-level_sem', 'B2-rank_B2-weapon_B3-level_diff_percentile_75-25', 'B2-rank_B2-weapon_B3-level_max', 'B2-rank_B2-weapon_B3-level_mean', 'B2-weapon_A2-level_diff_mean_x', 'B2-weapon_A3-level_percentile_25_x', 'B2-weapon_B1-level_ptp_x', 'B2-weapon_B2-level_hl_ratio_x', 'B2-weapon_B2-level_mean_x', 'B2-weapon_B3-level_var_x', 'B2-weapon_B4-level_max_x', 'B3-rank_B3-weapon_A1-level_ratio_mean', 'B3-rank_B3-weapon_A1-level_sem', 'B3-rank_B3-weapon_A1-level_skew', 'B3-rank_B3-weapon_A2-level_skew', 'B3-rank_B3-weapon_A2-level_sum', 'B3-rank_B3-weapon_A2-level_var', 'B3-rank_B3-weapon_A2-level_zscore', 'B3-rank_B3-weapon_A3-level_hl_ratio', 'B3-rank_B3-weapon_A3-level_ratio_mean', 'B3-rank_B3-weapon_A4-level_max', 'B3-rank_B3-weapon_A4-level_zscore', 'B3-rank_B3-weapon_B1-level_skew', 'B3-rank_B3-weapon_B1-level_zscore', 'B3-rank_B3-weapon_B2-level_kurt', 'B3-rank_B3-weapon_B3-level_diff_mean', 'B3-rank_B3-weapon_B3-level_skew', 'B3-rank_B3-weapon_B3-level_var', 'B3-rank_B3-weapon_B4-level_diff_percentile_75-25', 'B3-rank_B3-weapon_B4-level_hl_ratio', 'B3-rank_B3-weapon_B4-level_kurt', 'B3-weapon_A4-level_beyond1std_x', 'B3-weapon_B3-level_mad_x', 'B3-weapon_B4-level_beyond1std_x', 'B4-level', 'B4-rank_B4-weapon_A1-level_beyond1std', 'B4-rank_B4-weapon_A2-level_diff_mean', 'B4-rank_B4-weapon_A2-level_hl_ratio', 'B4-rank_B4-weapon_A2-level_mean', 'B4-rank_B4-weapon_A3-level_diff_mean', 'B4-rank_B4-weapon_A3-level_max', 'B4-rank_B4-weapon_A3-level_skew', 'B4-rank_B4-weapon_A3-level_zscore', 'B4-rank_B4-weapon_B1-level_sem', 'B4-rank_B4-weapon_B4-level_mad', 'B4-weapon_A1-level_ratio_range_x', 'B4-weapon_A1-level_sum_x', 'B4-weapon_A2-level_mad_x', 'B4-weapon_A3-level_kurt_x', 'B4-weapon_A3-level_ptp_x', 'B4-weapon_B4-level_max_x', 'B_ranks_A1-level_diff_mean', 'B_ranks_A2-level_mad', 'B_ranks_weapons_A1-level_max', 'B_ranks_weapons_A2-level_max', 'B_ranks_weapons_B2-level_min', 'B_weapons', 'B_weapons_A2-level_sum', 'B_weapons_B1-level_prod', 'B_weapons_B2-level_sum', 'B_weapons_B3-level_percentile_75', 'stage_B3-weapon_freq_entropy', 'y']\n",
    "    #df = df_all[cols]\n",
    "    \n",
    "    df = df_all\n",
    "\n",
    "    is_debug = False\n",
    "    # is_debug = True\n",
    "    if is_debug:\n",
    "        _df = df.head(1000)\n",
    "        _df = _df.append(df.tail(100))\n",
    "        df = _df\n",
    "\n",
    "    best_params = {'bagging_fraction': 0.9, 'bagging_freq': 6, 'feature_fraction': 0.1, 'max_depth': 7, 'min_child_samples': 343, 'min_child_weight': 0.04084861948055769, 'num_leaves': 95, 'reg_alpha': 0.5612212694825488, 'reg_lambda': 0.0001757886119766502}\n",
    "        \n",
    "    model = Model(OUT_MODEL)\n",
    "    lgb_params = dict(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.01,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        # verbose=1,\n",
    "        importance_type=\"gain\",\n",
    "        **best_params,\n",
    "    )\n",
    "    params = dict(\n",
    "        lgb_params=lgb_params,\n",
    "        df=df,\n",
    "        #num_folds=10,\n",
    "        num_folds=4,\n",
    "        target_col=\"y\",\n",
    "        del_cols=None,\n",
    "        eval_metric=\"error\",\n",
    "        #stratified=True,\n",
    "        stratified=False,\n",
    "        is_submission=True,\n",
    "        is_plot_perm_importance=False,\n",
    "    )\n",
    "    feat_importance, perm_importance = model.kfold_cv_LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメータチューニングしてモデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T03:54:11.896275Z",
     "start_time": "2020-08-26T00:41:52.991315Z"
    },
    "code_folding": [
     12
    ],
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "\n",
    "def exec_study(X, y, cv_index_kv, n_trials, output_dir, t_args):\n",
    "    def objective(trial):\n",
    "        gc.collect()\n",
    "        # ハイパーパラメータ\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 8)\n",
    "        num_leaves = trial.suggest_int(\"num_leaves\", 2, 2**max_depth)\n",
    "        min_child_samples = trial.suggest_int(\n",
    "            \"min_child_samples\", 1, max(1, int(len(cv_index_kv[1][0]) / num_leaves)))\n",
    "        tuning_params = dict(\n",
    "            max_depth=max_depth,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_samples=min_child_samples,\n",
    "            min_child_weight=trial.suggest_loguniform('min_child_weight', 0.001, 1000),\n",
    "            feature_fraction=trial.suggest_discrete_uniform('feature_fraction', 0.1, 0.95, 0.05),\n",
    "            bagging_fraction=trial.suggest_discrete_uniform('bagging_fraction', 0.4, 0.95, 0.05),\n",
    "            bagging_freq=trial.suggest_int('bagging_freq', 1, 10),\n",
    "            reg_alpha=trial.suggest_loguniform('reg_alpha', 1e-09, 10.0),\n",
    "            reg_lambda=trial.suggest_loguniform('reg_lambda', 1e-09, 10.0),\n",
    "        )\n",
    "        if t_args['objective'] == \"regression\":\n",
    "            tuning_params[\"reg_sqrt\"] = trial.suggest_categorical(\"reg_sqrt\", [True, False])\n",
    "        print(tuning_params)\n",
    "\n",
    "        # クロスバリデーション\n",
    "        def calc_score(train_index, val_index):\n",
    "            train_df = pd.concat([X, y], axis=1)\n",
    "            t_fold_df = train_df.iloc[train_index]\n",
    "            v_fold_df = train_df.iloc[val_index]\n",
    "            # カウントエンコディング\n",
    "            t_fold_df, v_fold_df = count_encoder(\n",
    "                t_fold_df, v_fold_df, cat_features=None\n",
    "            )\n",
    "            # ターゲットエンコディング\n",
    "            t_fold_df, v_fold_df = target_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # CatBoostエンコディング\n",
    "            t_fold_df, v_fold_df = catboost_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # ラベルエンコディング\n",
    "            cate_cols = t_fold_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                t_fold_df[col], uni = pd.factorize(t_fold_df[col])\n",
    "                v_fold_df[col], uni = pd.factorize(v_fold_df[col])\n",
    "            print(\n",
    "                \"run encoding Train shape: {}, valid shape: {}\".format(\n",
    "                    t_fold_df.shape, v_fold_df.shape\n",
    "                )\n",
    "            )\n",
    "            feats = t_fold_df.columns.to_list()\n",
    "            feats.remove(target_col)\n",
    "            X_train, y_train = (\n",
    "                t_fold_df[feats],\n",
    "                t_fold_df[target_col],\n",
    "            )\n",
    "            X_val, y_val = (\n",
    "                v_fold_df[feats],\n",
    "                v_fold_df[target_col],\n",
    "            )\n",
    "            \n",
    "            #X_train = X.iloc[train_index]  # TODO df,using_cols,gkfはグローバル変数にしないといけない？\n",
    "            #y_train = y.iloc[train_index]\n",
    "            #X_val   = X.iloc[val_index]\n",
    "            #y_val   = y.iloc[val_index]\n",
    "            if t_args['objective'] == \"regression\":\n",
    "                #model = lgb.LGBMRegressor(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)\n",
    "                model = lgb.LGBMRegressor(n_jobs=-1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)\n",
    "            else:\n",
    "                #model = lgb.LGBMClassifier(n_jobs=1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)\n",
    "                model = lgb.LGBMClassifier(n_jobs=-1, seed=71, n_estimators=10000, learning_rate=0.1, verbose=-1, **tuning_params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, eval_metric=t_args['eval_metric'], verbose=False)\n",
    "            \n",
    "            if t_args[\"objective\"] == \"regression\":\n",
    "                score = mean_squared_error(y_val, model.predict(X_val))\n",
    "            else:\n",
    "                #score = 1.0 - roc_auc_score(y_val, model.predict(X_val))\n",
    "                score = 1.0 - accuracy_score(y_val, model.predict(X_val))\n",
    "            return score\n",
    "        \n",
    "        #scores = Parallel(n_jobs=-1)([delayed(calc_score)(train_index, valid_index) for train_index, valid_index in cv_index_kv])\n",
    "        scores = []\n",
    "        for train_index, valid_index in cv_index_kv:\n",
    "            scores = calc_score(train_index, valid_index)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    # 学習実行\n",
    "    study = optuna.create_study(study_name=\"study\",\n",
    "                                storage=f\"sqlite:///{output_dir}/study.db\",\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"minimize\", \n",
    "                                sampler=optuna.samplers.TPESampler(seed=1))\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1, gc_after_trial=True)\n",
    "    \n",
    "    # 学習履歴保存\n",
    "    study.trials_dataframe().to_csv(f\"{output_dir}/study_history.csv\", index=False)\n",
    "    \n",
    "    # 最適化されたハイパーパラメータ\n",
    "    return study.best_params.copy()\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    df_all = pd.read_csv(f\"{WORK_DIR}/eda_preprocess.csv\", index_col=0)\n",
    "    # df_all = pd.read_csv(f\"{OUT_DATA}/feature_add.csv\")\n",
    "    # cols = pd.read_csv(f\"{OUT_DATA}/feature_selections.csv\")[\"feature_selections\"].values.tolist()\n",
    "    target_col = \"y\"\n",
    "    #cols.append(target_col)\n",
    "    #df = df_all[cols]\n",
    "    df = df_all   \n",
    "    print(df.shape)\n",
    "    \n",
    "    train = df[df[target_col].notnull()].reset_index(drop=True)\n",
    "    test = df[df[target_col].isnull()].reset_index(drop=True)\n",
    "    y_col = \"y\"\n",
    "    using_cols = df.columns.to_list()\n",
    "    using_cols = list(set(using_cols) - set([y_col]))\n",
    "    \n",
    "    n_trials = 300\n",
    "    \n",
    "    output_dir = OUT_MODEL\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    t_args = dict(objective=\"binary\",\n",
    "                  eval_metric=\"binary_logloss\",\n",
    "                  #objective=\"regression\",\n",
    "                  #eval_metric=\"rmse\",\n",
    "                 )\n",
    "    \n",
    "    X_train = df.loc[train.index][using_cols]\n",
    "    y_train = df.loc[train.index][y_col]\n",
    "    n_fold = 4\n",
    "    cv_index_kv = list(KFold(n_fold).split(X_train, y_train))\n",
    "    #cv_index_kv = list(StratifiedKFold(n_fold).split(X_train, y_train))\n",
    "    \n",
    "    # 学習実行\n",
    "    best_params = exec_study(X_train, y_train, cv_index_kv, n_trials, output_dir, t_args)\n",
    "    print(\"best_params:\\n\", best_params)\n",
    "    best_params_df = pd.DataFrame(best_params.values(), index=best_params.keys()) \n",
    "    best_params_df.to_csv(f\"{output_dir}/best_params.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:24:48.531177Z",
     "start_time": "2020-08-24T13:21:35.261793Z"
    },
    "code_folding": [
     6,
     250
    ],
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\Git\\OptGBM\")\n",
    "import optgbm as lgb\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, OUTPUT_DIR):\n",
    "        self.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "    # LightGBM GBDT with KFold or Stratified KFold\n",
    "    # Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "    def kfold_cv_LGBMClassifier(\n",
    "        self,\n",
    "        lgb_params: dict,\n",
    "        df: pd.DataFrame,\n",
    "        num_folds: int,\n",
    "        target_col: str,\n",
    "        del_cols=None,\n",
    "        eval_metric=\"error\",\n",
    "        stratified=True,  # StratifiedKFoldにするか\n",
    "        is_submission=False,  # Home_Credit_Default_Risk の submission.csv作成するか\n",
    "        is_plot_perm_importance=False,  # permutation importanceも出すか. feature_importance はデフォルトでだす\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LGBMClassifierでcross validation + feature_importance/permutation importance plot\n",
    "        \"\"\"\n",
    "        # Divide in training/validation and test data\n",
    "        train_df = df[df[target_col].notnull()].reset_index(drop=True)\n",
    "        test_df = df[df[target_col].isnull()].reset_index(drop=True)\n",
    "        print(\n",
    "            \"Starting LightGBM. Train shape: {}, test shape: {}\".format(\n",
    "                train_df.shape, test_df.shape\n",
    "            )\n",
    "        )\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        # Cross validation model\n",
    "        if stratified:\n",
    "            folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "        else:\n",
    "            folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "        # Create arrays and dataframes to store results\n",
    "        oof_preds = np.zeros(train_df.shape[0])\n",
    "        sub_preds = np.zeros(test_df.shape[0])\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        permutation_importance_df = pd.DataFrame()\n",
    "        result_scores = {}\n",
    "        train_probas = {}\n",
    "        test_probas = {}\n",
    "\n",
    "        # 目的変数とID列など削除\n",
    "        del_cols = del_cols.append(target_col) if del_cols is not None else [target_col]\n",
    "        feats = [f for f in train_df.columns if f not in del_cols]\n",
    "\n",
    "        for n_fold, (train_idx, valid_idx) in tqdm(\n",
    "            enumerate(folds.split(train_df[feats], train_df[target_col]))\n",
    "        ):\n",
    "            t_fold_df = train_df.iloc[train_idx]\n",
    "            v_fold_df = train_df.iloc[valid_idx]\n",
    "\n",
    "            # カウントエンコディング\n",
    "            t_fold_df, v_fold_df = count_encoder(\n",
    "                t_fold_df, v_fold_df, cat_features=None\n",
    "            )\n",
    "            # ターゲットエンコディング\n",
    "            t_fold_df, v_fold_df = target_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # CatBoostエンコディング\n",
    "            t_fold_df, v_fold_df = catboost_encoder(\n",
    "                t_fold_df, v_fold_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # ラベルエンコディング\n",
    "            cate_cols = t_fold_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                t_fold_df[col], uni = pd.factorize(t_fold_df[col])\n",
    "                v_fold_df[col], uni = pd.factorize(v_fold_df[col])\n",
    "            print(\n",
    "                \"run encoding Train shape: {}, valid shape: {}\".format(\n",
    "                    t_fold_df.shape, v_fold_df.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "            feats = t_fold_df.columns.to_list()\n",
    "            feats.remove(target_col)\n",
    "\n",
    "            train_x, train_y = (\n",
    "                t_fold_df[feats],\n",
    "                t_fold_df[target_col],\n",
    "            )\n",
    "            valid_x, valid_y = (\n",
    "                v_fold_df[feats],\n",
    "                v_fold_df[target_col],\n",
    "            )\n",
    "\n",
    "            ############################ train fit ############################\n",
    "            # LightGBM parameters found by Bayesian optimization\n",
    "            clf = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "            clf.fit(\n",
    "                train_x,\n",
    "                train_y,\n",
    "                # eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                # eval_metric=eval_metric,\n",
    "                # verbose=200,\n",
    "                # early_stopping_rounds=200,\n",
    "            )\n",
    "\n",
    "            # モデル保存\n",
    "            joblib.dump(clf, f\"{self.OUTPUT_DIR}/lgb-{n_fold + 1}.model\", compress=True)\n",
    "\n",
    "            # valid pred\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x, num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "\n",
    "            ############################ test pred ############################\n",
    "            # カウントエンコディング\n",
    "            tr_df, te_df = count_encoder(train_df, test_df, cat_features=None)\n",
    "            # ターゲットエンコディング\n",
    "            tr_df, te_df = target_encoder(\n",
    "                tr_df, te_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # CatBoostエンコディング\n",
    "            tr_df, te_df = catboost_encoder(\n",
    "                tr_df, te_df, target_col=target_col, cat_features=None\n",
    "            )\n",
    "            # ラベルエンコディング\n",
    "            cate_cols = tr_df.select_dtypes(\n",
    "                include=[\"object\", \"category\", \"bool\"]\n",
    "            ).columns.to_list()\n",
    "            for col in cate_cols:\n",
    "                tr_df[col], uni = pd.factorize(tr_df[col])\n",
    "                te_df[col], uni = pd.factorize(te_df[col])\n",
    "\n",
    "            # testの確信度\n",
    "            test_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(\n",
    "                te_df[feats], num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "            sub_preds += test_probas[f\"fold_{str(n_fold + 1)}\"] / folds.n_splits\n",
    "\n",
    "            # 一応trainの確信度も出しておく\n",
    "            train_probas[f\"fold_{str(n_fold + 1)}\"] = clf.predict_proba(\n",
    "                tr_df[feats], num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "\n",
    "            if eval_metric == \"auc\":\n",
    "                fold_auc = roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "                print(\"Fold %2d AUC : %.6f\" % (n_fold + 1, fold_auc))\n",
    "                result_scores[f\"fold_auc_{str(n_fold + 1)}\"] = fold_auc\n",
    "            elif eval_metric == \"error\":\n",
    "                # intにしないとaccuracy_score()エラーになる\n",
    "                _pred = oof_preds[valid_idx]\n",
    "                _pred[_pred >= 0.5] = 1\n",
    "                _pred[_pred < 0.5] = 0\n",
    "                fold_err = 1.0 - accuracy_score(valid_y, _pred)\n",
    "                print(\"Fold %2d error : %.6f\" % (n_fold + 1, fold_err))\n",
    "                result_scores[f\"fold_err_{str(n_fold + 1)}\"] = fold_err\n",
    "\n",
    "            # feature_importance\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat(\n",
    "                [feature_importance_df, fold_importance_df], axis=0\n",
    "            )\n",
    "\n",
    "            del clf, train_x, train_y, valid_x, valid_y\n",
    "            gc.collect()\n",
    "\n",
    "        if eval_metric == \"auc\":\n",
    "            mean_fold_auc = roc_auc_score(train_df[target_col], oof_preds)\n",
    "            print(\"Full AUC score %.6f\" % mean_fold_auc)\n",
    "            result_scores[\"mean_fold_auc\"] = mean_fold_auc\n",
    "        elif eval_metric == \"error\":\n",
    "            # intにしないとaccuracy_score()エラーになる\n",
    "            _pred = oof_preds\n",
    "            _pred[_pred >= 0.5] = 1\n",
    "            _pred[_pred < 0.5] = 0\n",
    "            mean_fold_err = 1.0 - accuracy_score(train_df[target_col], _pred)\n",
    "            print(\"Full error score %.6f\" % mean_fold_err)\n",
    "            result_scores[\"mean_fold_err\"] = mean_fold_err\n",
    "\n",
    "        # モデルのスコア出力\n",
    "        result_scores_df = pd.DataFrame(\n",
    "            result_scores.values(), index=result_scores.keys()\n",
    "        )\n",
    "        result_scores_df.to_csv(f\"{self.OUTPUT_DIR}/result_scores.tsv\", sep=\"\\t\")\n",
    "\n",
    "        test_probas_df = pd.DataFrame(test_probas)\n",
    "        test_probas_df.to_csv(f\"{self.OUTPUT_DIR}/test_probas.tsv\", index=False)\n",
    "        train_probas_df = pd.DataFrame(train_probas)\n",
    "        train_probas_df.to_csv(f\"{self.OUTPUT_DIR}/train_probas.tsv\", index=False)\n",
    "        # Write submission file (Home_Credit_Default_Risk)\n",
    "        if is_submission:\n",
    "            sub_preds[sub_preds >= 0.5] = 1\n",
    "            sub_preds[sub_preds < 0.5] = 0\n",
    "            test_df[target_col] = sub_preds\n",
    "            submission_file_name = f\"{self.OUTPUT_DIR}/submission_kernel.csv\"\n",
    "            sub_df = test_df[[target_col]]\n",
    "            sub_df[\"id\"] = test_df.index\n",
    "            sub_df.astype(int)\n",
    "            sub_df = sub_df[[\"id\", \"y\"]]\n",
    "            sub_df.to_csv(submission_file_name, index=False)\n",
    "\n",
    "        # Plot feature importance\n",
    "        Model(\"\").display_importances(\n",
    "            feature_importance_df,\n",
    "            png_path=f\"{self.OUTPUT_DIR}/lgbm_feature_importances.png\",\n",
    "            title=\"feature_importance\",\n",
    "        )\n",
    "        if is_plot_perm_importance:\n",
    "            Model(\"\").display_importances(\n",
    "                permutation_importance_df,\n",
    "                png_path=f\"{self.OUTPUT_DIR}/lgbm_permutation_importances.png\",\n",
    "                title=\"permutation_importance\",\n",
    "            )\n",
    "\n",
    "        return feature_importance_df, permutation_importance_df\n",
    "\n",
    "    # Display/plot feature/permutation importance\n",
    "    @staticmethod\n",
    "    def display_importances(\n",
    "        importance_df_, png_path, title,\n",
    "    ):\n",
    "        cols = (\n",
    "            importance_df_[[\"feature\", \"importance\"]]\n",
    "            .groupby(\"feature\")\n",
    "            .mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:40]\n",
    "            .index\n",
    "        )\n",
    "        best_features = importance_df_.loc[importance_df_.feature.isin(cols)]\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        sns.barplot(\n",
    "            x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False),\n",
    "        )\n",
    "        plt.title(f\"LightGBM {title} (avg over folds)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = df_all\n",
    "\n",
    "    # df_all = pd.read_csv(f\"{OUT_DATA}/feature_add.csv\")\n",
    "    # cols = pd.read_csv(f\"{OUT_DATA}/feature_selections.csv\")[\"feature_selections\"].values.tolist()\n",
    "    # cols.append(\"y\")\n",
    "    # df = df_all[cols]\n",
    "\n",
    "    is_debug = False\n",
    "    # is_debug = True\n",
    "    if is_debug:\n",
    "        _df = df.head(1000)\n",
    "        _df = _df.append(df.tail(100))\n",
    "        df = _df\n",
    "\n",
    "    model = Model(OUT_MODEL)\n",
    "    lgb_params = dict(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.01,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        # verbose=1,\n",
    "        importance_type=\"gain\",\n",
    "    )\n",
    "    params = dict(\n",
    "        lgb_params=lgb_params,\n",
    "        df=df,\n",
    "        num_folds=10,\n",
    "        target_col=\"y\",\n",
    "        del_cols=None,\n",
    "        eval_metric=\"error\",\n",
    "        stratified=True,\n",
    "        is_submission=True,\n",
    "        is_plot_perm_importance=False,\n",
    "    )\n",
    "    feat_importance, perm_importance = model.kfold_cv_LGBMClassifier(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
